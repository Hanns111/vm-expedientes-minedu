#!/usr/bin/env python3
"""
B√∫squeda Sem√°ntica Optimizada con FAISS IVF+PQ
==============================================

Implementaci√≥n de b√∫squeda vectorial ultrarr√°pida usando FAISS
con √≠ndices optimizados para producci√≥n.
"""

import faiss
import numpy as np
import pickle
import logging
import time
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
from sentence_transformers import SentenceTransformer
from prometheus_client import Counter, Histogram

from .cache_system import get_cache, cached

# M√©tricas FAISS
FAISS_SEARCH_OPERATIONS = Counter('faiss_search_operations_total', 'Total FAISS search operations', ['index_type'])
FAISS_SEARCH_DURATION = Histogram('faiss_search_duration_seconds', 'FAISS search duration', ['index_type'])
FAISS_INDEX_SIZE = Counter('faiss_index_size_vectors', 'Number of vectors in FAISS index', ['index_type'])

logger = logging.getLogger(__name__)

class OptimizedFAISSSearch:
    """
    B√∫squeda sem√°ntica optimizada usando FAISS con m√∫ltiples tipos de √≠ndices.
    
    Caracter√≠sticas:
    - √çndice IVF+PQ para datasets grandes (>10K vectores)
    - √çndice Flat para datasets peque√±os (<10K vectores)
    - Cache de embeddings precomputados
    - M√©tricas detalladas de rendimiento
    - B√∫squeda h√≠brida con fallback
    """
    
    def __init__(self, 
                 model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                 index_dir: str = 'data/faiss_indexes',
                 embedding_dim: int = 384):
        
        self.model_name = model_name
        self.embedding_dim = embedding_dim
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # Cargar modelo de embeddings
        try:
            self.model = SentenceTransformer(model_name)
            logger.info(f"‚úÖ Modelo de embeddings cargado: {model_name}")
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo: {e}")
            self.model = None
            
        # √çndices FAISS
        self.indexes = {}
        self.metadata = {}  # Metadatos asociados a cada vector
        self.cache = get_cache()
        
        # Configuraci√≥n de √≠ndices por tama√±o de dataset
        self.index_configs = {
            'small': {'type': 'Flat', 'threshold': 1000},
            'medium': {'type': 'IVF', 'threshold': 10000, 'nlist': 100},
            'large': {'type': 'IVFPQ', 'threshold': float('inf'), 'nlist': 1000, 'm': 64}
        }
        
        self._load_existing_indexes()
    
    def _load_existing_indexes(self):
        """Cargar √≠ndices existentes desde disco"""
        for index_file in self.index_dir.glob("*.faiss"):
            index_name = index_file.stem
            try:
                index = faiss.read_index(str(index_file))
                metadata_file = self.index_dir / f"{index_name}_metadata.pkl"
                
                if metadata_file.exists():
                    with open(metadata_file, 'rb') as f:
                        metadata = pickle.load(f)
                    
                    self.indexes[index_name] = index
                    self.metadata[index_name] = metadata
                    
                    FAISS_INDEX_SIZE.labels(index_type=self._get_index_type(index)).inc(index.ntotal)
                    logger.info(f"‚úÖ √çndice cargado: {index_name} ({index.ntotal} vectores)")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error cargando √≠ndice {index_name}: {e}")
    
    def _get_index_type(self, index) -> str:
        """Obtener tipo de √≠ndice FAISS"""
        if isinstance(index, faiss.IndexFlatIP):
            return 'Flat'
        elif isinstance(index, faiss.IndexIVFFlat):
            return 'IVF'
        elif isinstance(index, faiss.IndexIVFPQ):
            return 'IVFPQ'
        else:
            return 'Unknown'
    
    def _choose_index_config(self, num_vectors: int) -> Dict[str, Any]:
        """Elegir configuraci√≥n de √≠ndice seg√∫n tama√±o del dataset"""
        for config_name, config in self.index_configs.items():
            if num_vectors <= config['threshold']:
                return config
        return self.index_configs['large']
    
    def _create_index(self, vectors: np.ndarray, index_config: Dict[str, Any]) -> faiss.Index:
        """Crear √≠ndice FAISS optimizado"""
        num_vectors, dim = vectors.shape
        
        if index_config['type'] == 'Flat':
            # √çndice plano (exacto) para datasets peque√±os
            index = faiss.IndexFlatIP(dim)  # Inner Product (similitud coseno)
            
        elif index_config['type'] == 'IVF':
            # √çndice IVF para datasets medianos
            quantizer = faiss.IndexFlatIP(dim)
            index = faiss.IndexIVFFlat(quantizer, dim, index_config['nlist'])
            
        elif index_config['type'] == 'IVFPQ':
            # √çndice IVF+PQ para datasets grandes (compresi√≥n + velocidad)
            quantizer = faiss.IndexFlatIP(dim)
            index = faiss.IndexIVFPQ(quantizer, dim, index_config['nlist'], index_config['m'], 8)
            
        else:
            raise ValueError(f"Tipo de √≠ndice no soportado: {index_config['type']}")
        
        # Normalizar vectores para similitud coseno
        faiss.normalize_L2(vectors)
        
        # Entrenar √≠ndice si es necesario
        if hasattr(index, 'train'):
            logger.info(f"üèãÔ∏è Entrenando √≠ndice {index_config['type']} con {num_vectors} vectores...")
            index.train(vectors)
        
        # Agregar vectores al √≠ndice
        index.add(vectors)
        
        logger.info(f"‚úÖ √çndice {index_config['type']} creado: {num_vectors} vectores, {dim}D")
        return index
    
    @cached('hybrid', ttl=3600)
    def create_index_from_documents(self, 
                                  documents: List[str], 
                                  index_name: str,
                                  metadata_list: Optional[List[Dict[str, Any]]] = None) -> bool:
        """
        Crear √≠ndice FAISS desde lista de documentos
        
        Args:
            documents: Lista de textos para indexar
            index_name: Nombre del √≠ndice
            metadata_list: Metadatos asociados a cada documento
            
        Returns:
            True si el √≠ndice se cre√≥ exitosamente
        """
        start_time = time.time()
        
        try:
            if not self.model:
                raise ValueError("Modelo de embeddings no disponible")
                
            logger.info(f"üî® Creando √≠ndice '{index_name}' con {len(documents)} documentos...")
            
            # Generar embeddings
            logger.info("üß† Generando embeddings...")
            embeddings = self.model.encode(documents, show_progress_bar=True)
            embeddings = np.array(embeddings, dtype=np.float32)
            
            # Elegir configuraci√≥n de √≠ndice
            index_config = self._choose_index_config(len(documents))
            logger.info(f"üìä Configuraci√≥n elegida: {index_config['type']} para {len(documents)} vectores")
            
            # Crear √≠ndice
            index = self._create_index(embeddings, index_config)
            
            # Guardar √≠ndice y metadatos
            index_path = self.index_dir / f"{index_name}.faiss"
            faiss.write_index(index, str(index_path))
            
            if metadata_list:
                metadata_path = self.index_dir / f"{index_name}_metadata.pkl"
                with open(metadata_path, 'wb') as f:
                    pickle.dump(metadata_list, f)
                self.metadata[index_name] = metadata_list
            
            # Guardar en memoria
            self.indexes[index_name] = index
            
            # M√©tricas
            creation_time = time.time() - start_time
            FAISS_INDEX_SIZE.labels(index_type=index_config['type']).inc(len(documents))
            
            logger.info(f"‚úÖ √çndice '{index_name}' creado en {creation_time:.2f}s")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error creando √≠ndice '{index_name}': {e}")
            return False
    
    def search(self, 
               query: str, 
               index_name: str, 
               k: int = 5,
               score_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """
        B√∫squeda sem√°ntica en √≠ndice FAISS
        
        Args:
            query: Consulta de b√∫squeda
            index_name: Nombre del √≠ndice a usar
            k: N√∫mero de resultados a devolver
            score_threshold: Umbral m√≠nimo de similitud
            
        Returns:
            Lista de resultados con scores y metadatos
        """
        start_time = time.time()
        
        try:
            if index_name not in self.indexes:
                logger.warning(f"‚ö†Ô∏è √çndice '{index_name}' no encontrado")
                return []
                
            if not self.model:
                logger.error("‚ùå Modelo de embeddings no disponible")
                return []
            
            index = self.indexes[index_name]
            metadata = self.metadata.get(index_name, [])
            
            # Generar embedding de la consulta
            query_embedding = self.model.encode([query])
            query_embedding = np.array(query_embedding, dtype=np.float32)
            faiss.normalize_L2(query_embedding)  # Normalizar para similitud coseno
            
            # B√∫squeda en FAISS
            scores, indices = index.search(query_embedding, k)
            
            # Procesar resultados
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx == -1:  # FAISS devuelve -1 si no encuentra suficientes resultados
                    continue
                    
                if score < score_threshold:
                    continue
                
                result = {
                    'rank': i + 1,
                    'score': float(score),
                    'document_id': int(idx),
                    'metadata': metadata[idx] if idx < len(metadata) else {}
                }
                results.append(result)
            
            # M√©tricas
            search_time = time.time() - start_time
            index_type = self._get_index_type(index)
            
            FAISS_SEARCH_OPERATIONS.labels(index_type=index_type).inc()
            FAISS_SEARCH_DURATION.labels(index_type=index_type).observe(search_time)
            
            logger.debug(f"üîç B√∫squeda FAISS completada: {len(results)} resultados en {search_time:.3f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Error en b√∫squeda FAISS: {e}")
            return []
    
    def get_index_stats(self, index_name: str) -> Dict[str, Any]:
        """Obtener estad√≠sticas de un √≠ndice espec√≠fico"""
        if index_name not in self.indexes:
            return {'error': f"√çndice '{index_name}' no encontrado"}
        
        index = self.indexes[index_name]
        metadata = self.metadata.get(index_name, [])
        
        return {
            'index_name': index_name,
            'index_type': self._get_index_type(index),
            'total_vectors': index.ntotal,
            'dimension': index.d,
            'metadata_entries': len(metadata),
            'is_trained': getattr(index, 'is_trained', True),
            'memory_usage_mb': self._estimate_memory_usage(index)
        }
    
    def _estimate_memory_usage(self, index) -> float:
        """Estimar uso de memoria de un √≠ndice FAISS"""
        try:
            # Estimaci√≥n b√°sica basada en tipo de √≠ndice
            if isinstance(index, faiss.IndexFlatIP):
                return (index.ntotal * index.d * 4) / (1024 * 1024)  # 4 bytes por float32
            elif isinstance(index, faiss.IndexIVFFlat):
                return (index.ntotal * index.d * 4) / (1024 * 1024) * 1.2  # +20% overhead
            elif isinstance(index, faiss.IndexIVFPQ):
                # PQ usa menos memoria debido a compresi√≥n
                return (index.ntotal * index.d * 4) / (1024 * 1024) * 0.25  # ~75% compresi√≥n
            else:
                return 0.0
        except:
            return 0.0
    
    def list_indexes(self) -> List[Dict[str, Any]]:
        """Listar todos los √≠ndices disponibles"""
        return [self.get_index_stats(name) for name in self.indexes.keys()]
    
    def delete_index(self, index_name: str) -> bool:
        """Eliminar √≠ndice de memoria y disco"""
        try:
            # Remover de memoria
            if index_name in self.indexes:
                del self.indexes[index_name]
            if index_name in self.metadata:
                del self.metadata[index_name]
            
            # Remover archivos
            index_path = self.index_dir / f"{index_name}.faiss"
            metadata_path = self.index_dir / f"{index_name}_metadata.pkl"
            
            if index_path.exists():
                index_path.unlink()
            if metadata_path.exists():
                metadata_path.unlink()
                
            logger.info(f"üóëÔ∏è √çndice '{index_name}' eliminado")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error eliminando √≠ndice '{index_name}': {e}")
            return False
    
    def optimize_index(self, index_name: str) -> bool:
        """Optimizar √≠ndice existente (re-entrenar con mejores par√°metros)"""
        try:
            if index_name not in self.indexes:
                logger.warning(f"‚ö†Ô∏è √çndice '{index_name}' no encontrado para optimizar")
                return False
            
            # Para simplificar, esta implementaci√≥n recrea el √≠ndice
            # En producci√≥n, se podr√≠an aplicar optimizaciones espec√≠ficas
            logger.info(f"üîß Optimizaci√≥n de √≠ndice '{index_name}' no implementada a√∫n")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error optimizando √≠ndice '{index_name}': {e}")
            return False

# Factory function para uso f√°cil
def create_faiss_search(model_name: str = None) -> OptimizedFAISSSearch:
    """Crear instancia de b√∫squeda FAISS con configuraci√≥n por defecto"""
    if model_name is None:
        model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    
    return OptimizedFAISSSearch(model_name=model_name)