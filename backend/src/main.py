"""
FastAPI application with plugin system and multi-LLM router.
"""

from fastapi import FastAPI, HTTPException, Depends, Security
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from contextlib import asynccontextmanager
import uvicorn
import logging
import time
import sys
import os
import re

# === LOGGING PARA SEGURIDAD DE PRODUCCIÃ“N ===
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from uuid import UUID

# Setup paths
backend_dir = Path(__file__).parent.parent  # /backend
project_root = backend_dir.parent  # /vm-expedientes-minedu
sys.path.insert(0, str(project_root))  # Add project root to path

# Direct import - create ProjectPaths inline to avoid import issues
class ProjectPaths:
    """Project paths configuration."""
    
    PROJECT_ROOT: Path = Path(__file__).resolve().parents[3]
    
    # Directories
    DATA_DIR: Path = PROJECT_ROOT / "data"
    VECTORSTORES_DIR: Path = DATA_DIR / "vectorstores"

    # Vectorstores
    BM25_VECTORSTORE: Path = VECTORSTORES_DIR / "bm25.pkl"
    TFIDF_VECTORSTORE: Path = VECTORSTORES_DIR / "tfidf.pkl"
    TRANSFORMERS_VECTORSTORE: Path = VECTORSTORES_DIR / "transformers.pkl"

    @classmethod
    def rel(cls, path: Path) -> str:
        """Get relative path from project root."""
        try:
            return str(path.relative_to(cls.PROJECT_ROOT))
        except ValueError:
            return str(path)

# ==================== LANGGRAPH PROFESIONAL INTEGRATION ====================
# NUEVO: LangGraph PROFESIONAL con validaciÃ³n, retry y observabilidad
try:
    from backend.src.langchain_integration.vectorstores.simple_retriever import retriever
    print(f"ğŸ“Š SimpleRetriever cargado: {retriever.get_stats()['total_documents']} documentos")
    
    # Try to import LangGraph components
    from backend.src.langchain_integration.orchestration.professional_langgraph import professional_orchestrator
    from backend.src.langchain_integration.orchestration.real_langgraph import real_orchestrator
    from backend.src.langchain_integration.config import config as langchain_config
    LANGGRAPH_PROFESSIONAL_AVAILABLE = True
    LANGGRAPH_REAL_AVAILABLE = True
    print("ğŸš€ LangGraph PROFESIONAL cargado exitosamente - Con validaciÃ³n, retry y fallback")
    print("ğŸ¯ Sistema: NIVEL EMPRESARIAL")
except ImportError as e:
    print(f"âš ï¸ LangGraph no disponible (usando fallback): {e}")
    try:
        # Load only SimpleRetriever for fallback
        from backend.src.langchain_integration.vectorstores.simple_retriever import retriever
        print(f"âœ… Fallback con SimpleRetriever: {retriever.get_stats()['total_documents']} documentos")
    except ImportError as fallback_error:
        print(f"âŒ Fallback tambiÃ©n fallÃ³: {fallback_error}")
        retriever = None
    
    LANGGRAPH_PROFESSIONAL_AVAILABLE = False
    LANGGRAPH_REAL_AVAILABLE = False
    professional_orchestrator = None
    real_orchestrator = None

# ANTIALUCINACIONES v2.0.0: Simuladores eliminados completamente
# PROHIBIDO: Importar o usar simuladores en sistema gubernamental
LANGCHAIN_SIMULATOR_AVAILABLE = False  # Permanentemente deshabilitado
orchestrator = None  # Solo se usa real_orchestrator

# Mantener sistema hÃ­brido existente como fallback
try:
    from .core.hybrid.hybrid_search import HybridSearch
    HYBRID_SEARCH_AVAILABLE = True
    print("âœ… Sistema hÃ­brido legacy disponible como fallback")
except ImportError as e:
    print(f"âš ï¸ Hybrid search not available: {e}")
    print(f"Project root: {project_root}")
    print(f"Backend dir: {backend_dir}")
    HYBRID_SEARCH_AVAILABLE = False
    HybridSearch = None  # Define it as None so no NameError

# Import backend core modules with error handling
try:
    from .core.plugins.plugin_registry import PluginRegistry
except ImportError:
    PluginRegistry = None

# Import legal reasoning
try:
    sys.path.append(str(project_root))
    from backend.src.domain.legal_reasoning import create_legal_reasoner
    legal_reasoner = create_legal_reasoner()
    LEGAL_REASONING_AVAILABLE = True
    print("âš–ï¸ Legal reasoning engine loaded")
except ImportError as e:
    print(f"âš ï¸ Legal reasoning not available: {e}")
    legal_reasoner = None
    LEGAL_REASONING_AVAILABLE = False

# ENTERPRISE FEATURES: Auth, Feedback, Database
try:
    from .core.auth.jwt_auth import JWTAuth, get_current_user, User
    from .core.feedback.feedback_system import FeedbackRequest, submit_user_feedback, get_system_feedback_analytics
    from .core.database.session import get_async_session
    from .core.reranking.advanced_reranker import global_reranker, initialize_reranker
    ENTERPRISE_FEATURES_AVAILABLE = True
    print("ğŸ¢ Enterprise features loaded: JWT Auth, Feedback System, Advanced Reranking")
except ImportError as e:
    print(f"âš ï¸ Enterprise features not available: {e}")
    ENTERPRISE_FEATURES_AVAILABLE = False
    get_current_user = None
    get_async_session = None

try:
    from .core.llm.model_router import ModelRouter  
except ImportError:
    ModelRouter = None

try:
    from .core.config.settings import get_settings
except ImportError:
    # TODO MINOR: Fallback settings - implementar configuraciÃ³n real cuando sea necesario
    class MockSettings:
        cors_origins = ["http://localhost:3000", "http://localhost:3001"]
    def get_settings():
        return MockSettings()
    print("âš ï¸ Using fallback settings")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global instances
plugin_registry: PluginRegistry = None
model_router: ModelRouter = None
hybrid_search: HybridSearch = None
settings = get_settings()

# ==================== LEGAL REASONING ENGINE ====================
try:
    from .domain.legal_reasoning import create_legal_reasoner
    legal_reasoner = create_legal_reasoner()
    LEGAL_REASONING_AVAILABLE = True
    print("âœ… Legal reasoning engine loaded successfully")
except Exception as e:
    legal_reasoner = None
    LEGAL_REASONING_AVAILABLE = False
    print(f"âš ï¸ Legal reasoning not available: {e}")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management."""
    global plugin_registry, model_router, hybrid_search
    
    # Startup
    logger.info("ğŸš€ Starting Government AI Platform...")
    try:
        if PluginRegistry:
            plugin_registry = PluginRegistry("../config/plugins.yaml")
            logger.info(f"âœ… Loaded {len(plugin_registry.plugins)} plugins")
        else:
            logger.info("ğŸ“ Plugin registry not available - using mock data")
            
        if ModelRouter:
            model_router = ModelRouter("../config/models.yaml")
            logger.info(f"âœ… Loaded {len(model_router.models)} models")
        else:
            logger.info("ğŸ“ Model router not available - using mock data")
    except Exception as e:
        logger.error(f"âŒ Error during startup: {e}")
        # Continue with mock data for development
        logger.info("ğŸ“ Using mock data for development")
    
    # Initialize Hybrid Search
    if HYBRID_SEARCH_AVAILABLE:
        try:
            # Paths to vectorstores from project root
            bm25_path = str(ProjectPaths.BM25_VECTORSTORE)
            tfidf_path = str(ProjectPaths.TFIDF_VECTORSTORE)
            transformer_path = str(ProjectPaths.TRANSFORMERS_VECTORSTORE)
            
            print(f"ğŸ” Loading vectorstores from:")
            print(f"  - BM25: {bm25_path}")
            print(f"  - TF-IDF: {tfidf_path}")
            print(f"  - Transformer: {transformer_path}")
            
            hybrid_search = HybridSearch(
                bm25_vectorstore_path=bm25_path,
                tfidf_vectorstore_path=tfidf_path,
                transformer_vectorstore_path=transformer_path,
                fusion_strategy='weighted'
            )
            logger.info("âœ… Hybrid search system initialized")
        except Exception as e:
            logger.error(f"âŒ Error initializing hybrid search: {e}")
            hybrid_search = None
    else:
        logger.warning("âŒ Hybrid search not available")
        hybrid_search = None
    
    yield
    
    # Shutdown
    logger.info("ğŸ”„ Shutting down Government AI Platform...")

# Create FastAPI app
app = FastAPI(
    title="Government AI Platform API",
    description="Sistema de IA para procesamiento de documentos gubernamentales",
    version="2.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Global exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "timestamp": datetime.now().isoformat()}
    )

@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "message": "Government AI Platform API v2.0",
        "status": "active",
        "timestamp": datetime.now().isoformat(),
        "docs": "/docs"
    }

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    try:
        plugins_count = len(plugin_registry.plugins) if plugin_registry else 0
        models_count = len(model_router.models) if model_router else 0
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "plugins_loaded": plugins_count,
            "models_loaded": models_count,
            "version": "2.0.0",
            "uptime": "active"
        }
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return {
            "status": "degraded",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }

@app.get("/api/admin/plugins")
async def get_plugins():
    """Get all plugins configuration."""
    try:
        if plugin_registry and plugin_registry.plugins:
            return [
                {
                    "id": plugin.id,
                    "name": plugin.name,
                    "description": plugin.description,
                    "enabled": plugin.enabled,
                    "capabilities": plugin.capabilities,
                    "version": plugin.version,
                    "metrics": {
                        "total_requests": plugin.metrics.get("total_requests", 0),
                        "success_rate": plugin.metrics.get("success_rate", 100.0),
                        "avg_latency": plugin.metrics.get("avg_latency", 200),
                        "errors_last_hour": plugin.metrics.get("errors_last_hour", 0)
                    }
                }
                for plugin in plugin_registry.plugins.values()
            ]
        else:
            # TODO MINOR: Mock data para desarrollo - implementar datos reales
            return [
                {
                    "id": "audio-transcription",
                    "name": "TranscripciÃ³n de Audio",
                    "description": "Convierte audio a texto usando Whisper",
                    "enabled": True,
                    "capabilities": ["audio_processing"],
                    "version": "1.0.0",
                    "metrics": {
                        "total_requests": 156,
                        "success_rate": 98.7,
                        "avg_latency": 2500,
                        "errors_last_hour": 0
                    }
                },
                {
                    "id": "document-ocr",
                    "name": "OCR de Documentos",
                    "description": "Extrae texto de imÃ¡genes y PDFs",
                    "enabled": True,
                    "capabilities": ["vision_processing", "document_analysis"],
                    "version": "1.2.0",
                    "metrics": {
                        "total_requests": 342,
                        "success_rate": 95.2,
                        "avg_latency": 1800,
                        "errors_last_hour": 2
                    }
                },
                {
                    "id": "entity-extraction",
                    "name": "ExtracciÃ³n de Entidades",
                    "description": "Identifica y extrae entidades especÃ­ficas de documentos",
                    "enabled": True,
                    "capabilities": ["nlp_processing", "entity_recognition"],
                    "version": "1.1.0",
                    "metrics": {
                        "total_requests": 789,
                        "success_rate": 92.1,
                        "avg_latency": 650,
                        "errors_last_hour": 1
                    }
                }
            ]
    except Exception as e:
        logger.error(f"Error getting plugins: {e}")
        raise HTTPException(status_code=500, detail="Error retrieving plugins")

@app.get("/api/admin/models")
async def get_models():
    """Get all models configuration."""
    try:
        if model_router and model_router.models:
            return [
                {
                    "model_name": model.model_name,
                    "display_name": model.display_name,
                    "description": model.description,
                    "enabled": model.enabled,
                    "provider": model.provider,
                    "cost_per_1k_tokens": model.cost_per_1k_tokens,
                    "capabilities": model.capabilities,
                    "metrics": {
                        "total_requests": model.metrics.get("total_requests", 0),
                        "total_tokens": model.metrics.get("total_tokens", 0),
                        "avg_latency": model.latency_ms,
                        "success_rate": model.metrics.get("success_rate", 100.0)
                    }
                }
                for model in model_router.models.values()
            ]
        else:
            # TODO MINOR: Mock data para desarrollo - implementar datos reales
            return [
                {
                    "model_name": "gpt-4-turbo",
                    "display_name": "GPT-4 Turbo",
                    "description": "Modelo avanzado para anÃ¡lisis complejos",
                    "enabled": True,
                    "provider": "openai",
                    "cost_per_1k_tokens": 0.03,
                    "capabilities": ["reasoning", "analysis", "multilingual"],
                    "metrics": {
                        "total_requests": 1250,
                        "total_tokens": 45000,
                        "avg_latency": 2000,
                        "success_rate": 99.1
                    }
                },
                {
                    "model_name": "claude-3-sonnet",
                    "display_name": "Claude 3 Sonnet",
                    "description": "Modelo equilibrado con alta precisiÃ³n",
                    "enabled": True,
                    "provider": "anthropic",
                    "cost_per_1k_tokens": 0.015,
                    "capabilities": ["reasoning", "analysis", "safety"],
                    "metrics": {
                        "total_requests": 890,
                        "total_tokens": 32000,
                        "avg_latency": 1800,
                        "success_rate": 97.8
                    }
                },
                {
                    "model_name": "gpt-3.5-turbo",
                    "display_name": "GPT-3.5 Turbo",
                    "description": "Modelo rÃ¡pido para consultas generales",
                    "enabled": True,
                    "provider": "openai",
                    "cost_per_1k_tokens": 0.002,
                    "capabilities": ["fast_response", "general_queries"],
                    "metrics": {
                        "total_requests": 2100,
                        "total_tokens": 67000,
                        "avg_latency": 1200,
                        "success_rate": 98.5
                    }
                }
            ]
    except Exception as e:
        logger.error(f"Error getting models: {e}")
        raise HTTPException(status_code=500, detail="Error retrieving models")

@app.get("/api/admin/system-status")
async def get_system_status():
    """Get overall system status and metrics."""
    try:
        return {
            "status": "operational",
            "timestamp": datetime.now().isoformat(),
            "uptime": "99.9%",
            "active_connections": 42,
            "total_requests_today": 5420,
            "average_response_time": 1250,
            "error_rate": 0.8,
            "system_load": {
                "cpu_usage": 35.2,
                "memory_usage": 68.7,
                "disk_usage": 45.1
            },
            "services": {
                "database": "healthy",
                "redis": "healthy",
                "llm_router": "healthy",
                "plugin_registry": "healthy"
            }
        }
    except Exception as e:
        logger.error(f"Error getting system status: {e}")
        raise HTTPException(status_code=500, detail="Error retrieving system status")

@app.post("/api/admin/plugins/{plugin_id}/toggle")
async def toggle_plugin(plugin_id: str):
    """Toggle plugin enabled/disabled state."""
    try:
        # In a real implementation, this would update the plugin state
        return {
            "plugin_id": plugin_id,
            "action": "toggled",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error toggling plugin {plugin_id}: {e}")
        raise HTTPException(status_code=500, detail="Error toggling plugin")

@app.post("/api/admin/models/{model_name}/toggle")
async def toggle_model(model_name: str):
    """Toggle model enabled/disabled state."""
    try:
        # In a real implementation, this would update the model state
        return {
            "model_name": model_name,
            "action": "toggled",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error toggling model {model_name}: {e}")
        raise HTTPException(status_code=500, detail="Error toggling model")

# Chat and Search Endpoints
from pydantic import BaseModel

class ChatRequest(BaseModel):
    message: str
    conversation_id: str = None

class SearchRequest(BaseModel):
    query: str
    method: str = "hybrid"
    top_k: int = 5

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """Chat endpoint for conversational AI with real hybrid search."""
    try:
        logger.info(f"Received chat request: {request.message}")
        start_time = time.time()
        
        # Check if hybrid search is available
        if hybrid_search is None:
            logger.warning("Hybrid search not available, using fallback response")
            return _generate_fallback_response(request)
        
        # Perform hybrid search
        try:
            search_results = hybrid_search.search(request.message, top_k=5)
            logger.info(f"Hybrid search returned {len(search_results)} results")
            
            # Generate response based on search results
            response_data = await _generate_chat_response(request, search_results, start_time)
            
            logger.info(f"Generated response for query: '{request.message[:50]}...'")
            return response_data
            
        except Exception as search_error:
            logger.error(f"Error in hybrid search: {search_error}")
            return _generate_fallback_response(request, error_msg=str(search_error))
        
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}")
        raise HTTPException(status_code=500, detail="Error processing chat request")

async def _generate_chat_response(request: ChatRequest, search_results: List[Dict], start_time: float) -> Dict:
    """Generate chat response using LangChain orchestrator or fallback to legacy system."""
    processing_time = time.time() - start_time
    
    # ==================== PARCHE: USAR RETRIEVER DIRECTO SI LANGGRAPH NO DISPONIBLE ====================
    # NUEVO: Si SimpleRetriever estÃ¡ disponible, usar directamente (parche sin LangGraph)
    if not LANGGRAPH_PROFESSIONAL_AVAILABLE and retriever:
        try:
            logger.info("ğŸ”§ Usando SimpleRetriever directo (parche sin LangGraph)")
            
            # BÃºsqueda directa con SimpleRetriever
            documents = retriever.simple_similarity_search(request.message, k=5)
            
            if documents:
                # Generar respuesta basada en documentos reales
                context_chunks = []
                for doc in documents:
                    context_chunks.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "confidence": 0.8
                    })
                
                # Respuesta estructurada basada en documentos reales
                response_parts = [
                    f"ğŸ“‹ **BÃšSQUEDA EN DOCUMENTOS MINEDU**",
                    f"ğŸ” **Consulta**: {request.message}",
                    f"ğŸ“Š **Documentos encontrados**: {len(documents)}",
                    "",
                    "ğŸ“„ **INFORMACIÃ“N ENCONTRADA**:"
                ]
                
                for i, doc in enumerate(documents[:3], 1):
                    content_preview = doc.page_content[:200].replace('\n', ' ')
                    response_parts.append(f"**{i}.** {content_preview}...")
                
                # Integrar razonamiento legal si estÃ¡ disponible
                if LEGAL_REASONING_AVAILABLE and legal_reasoner:
                    try:
                        legal_analysis = legal_reasoner.provide_legal_reasoning(
                            request.message,
                            [{"content": doc.page_content, "title": doc.metadata.get("titulo", "")} for doc in documents]
                        )
                        response_parts.append("")
                        response_parts.append(legal_analysis)
                    except Exception as le:
                        logger.error(f"Error in legal reasoning: {le}")
                
                response_text = "\n".join(response_parts)
                
                return {
                    "response": response_text,
                    "conversation_id": request.conversation_id or f"retriever_{int(time.time())}",
                    "timestamp": datetime.now().isoformat(),
                    "sources": context_chunks,
                    "processing_time": round(processing_time, 3),
                    "method": "simple_retriever_direct",
                    "documents_found": len(documents),
                    "system": "RAG_REAL"
                }
            else:
                logger.warning("No documents found with SimpleRetriever")
                
        except Exception as retriever_error:
            logger.error(f"Error in SimpleRetriever: {retriever_error}")
    
    # ==================== LANGGRAPH PROFESIONAL (si disponible) ====================
    # Usar LangGraph PROFESIONAL si estÃ¡ disponible
    if LANGGRAPH_PROFESSIONAL_AVAILABLE and professional_orchestrator:
        try:
            logger.info("ğŸš€ Usando LangGraph PROFESIONAL - ValidaciÃ³n + Retry + Fallback")
            
            # Procesar con LangGraph PROFESIONAL
            thread_id = request.conversation_id or f"prof_{int(time.time())}"
            professional_result = await professional_orchestrator.process_query_professional(
                request.message, 
                thread_id=thread_id
            )
            
            if professional_result and not professional_result.get("error"):
                logger.info("âœ… LangGraph PROFESIONAL exitoso - sistema empresarial")
                # Integrar razonamiento legal, si estÃ¡ disponible y hay documentos
                if LEGAL_REASONING_AVAILABLE and legal_reasoner and professional_result.get("sources"):
                    try:
                        legal_analysis = legal_reasoner.provide_legal_reasoning(
                            request.message,
                            professional_result.get("sources", [])
                        )
                        professional_result["response"] = f"{professional_result.get('response', '')}\n\n{legal_analysis}"
                        professional_result["legal_reasoning"] = True
                    except Exception as le:
                        logger.error(f"Error applying legal reasoning in professional flow: {le}")
                        professional_result["legal_reasoning"] = False
                return {
                    "response": professional_result.get("response", ""),
                    "conversation_id": thread_id,
                    "timestamp": datetime.now().isoformat(),
                    "sources": professional_result.get("sources", []),
                    "processing_time": round(processing_time, 3),
                    "method": "langgraph_professional",
                    "intent": professional_result.get("intent", ""),
                    "agent_used": professional_result.get("agent_used", ""),
                    "documents_found": professional_result.get("documents_found", 0),
                    "confidence": professional_result.get("confidence", 0.8),
                    "orchestrator_info": professional_result.get("orchestrator_info", {}),
                    "extracted_info": professional_result.get("extracted_info", {}),
                    "system": "langgraph_professional"
                }
            else:
                logger.warning("âš ï¸ LangGraph PROFESIONAL fallÃ³, probando LangGraph bÃ¡sico")
        
        except Exception as e:
            logger.error(f"âŒ Error en LangGraph PROFESIONAL: {e}")
    
    # ==================== FALLBACK: LANGGRAPH REAL ====================
    # Fallback a LangGraph REAL si el profesional falla
    if LANGGRAPH_REAL_AVAILABLE and real_orchestrator:
        try:
            logger.info("ğŸ”„ Fallback: Usando LangGraph REAL bÃ¡sico")
            
            # Procesar con LangGraph REAL
            thread_id = request.conversation_id or f"real_{int(time.time())}"
            langgraph_result = await real_orchestrator.process_query_real(
                request.message, 
                thread_id=thread_id
            )
            
            if langgraph_result and not langgraph_result.get("error"):
                logger.info("âœ… LangGraph REAL exitoso como fallback")
                return {
                    "response": langgraph_result.get("response", ""),
                    "conversation_id": thread_id,
                    "timestamp": datetime.now().isoformat(),
                    "sources": langgraph_result.get("sources", []),
                    "processing_time": round(processing_time, 3),
                    "method": "langgraph_real_fallback",
                    "intent": langgraph_result.get("intent", ""),
                    "agent_used": langgraph_result.get("agent_used", ""),
                    "documents_found": langgraph_result.get("documents_found", 0),
                    "confidence": langgraph_result.get("confidence", 0.8),
                    "orchestrator_info": langgraph_result.get("orchestrator_info", {}),
                    "system": "langgraph_real"
                }
        
        except Exception as e:
            logger.error(f"âŒ Error en LangGraph REAL fallback: {e}")
    
    # ==================== FALLBACK: SIMULADOR LANGCHAIN ====================
    # Fallback al simulador SI LangGraph REAL falla
    if LANGCHAIN_SIMULATOR_AVAILABLE and orchestrator:
        try:
            logger.info("ğŸ”„ Fallback: Usando simulador LangChain")
            
            # Procesar con simulador
            langchain_result = await orchestrator.process_query(request.message)
            
            if langchain_result and not langchain_result.get("error"):
                logger.info("âœ… Simulador LangChain exitoso como fallback")
                return {
                    "response": langchain_result.get("response", ""),
                    "conversation_id": request.conversation_id or f"conv_{int(time.time())}",
                    "timestamp": datetime.now().isoformat(),
                    "sources": langchain_result.get("sources", []),
                    "processing_time": round(processing_time, 3),
                    "method": "langchain_simulator_fallback",
                    "intent": langchain_result.get("intent", ""),
                    "agent_used": langchain_result.get("agent_used", ""),
                    "documents_found": langchain_result.get("documents_found", 0),
                    "confidence": langchain_result.get("confidence", 0.8),
                    "orchestrator_info": langchain_result.get("orchestrator_info", {}),
                    "system": "langchain_simulator"
                }
            
        except Exception as e:
            logger.error(f"âŒ Error en simulador LangChain: {e}")
    
    # ==================== FALLBACK: SISTEMA LEGACY ====================
    logger.info("ğŸ“ Usando sistema hÃ­brido legacy como fallback")
    
    if not search_results:
        return {
            "response": "No encontrÃ© informaciÃ³n especÃ­fica sobre tu consulta en los documentos disponibles. "
                       "Â¿PodrÃ­as reformular tu pregunta o ser mÃ¡s especÃ­fico?",
            "conversation_id": request.conversation_id or f"conv_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "sources": [],
            "processing_time": processing_time
        }
    
    # Extract the best content from search results
    best_result = search_results[0] if search_results else None
    
    # Improved intent detection and response generation
    query_lower = request.message.lower()
    intent = _detect_query_intent(query_lower)
    
    # Generate response based on detected intent
    if intent == "montos_maximos":
        response_text = _generate_montos_maximos_response(query_lower, search_results)
    elif intent == "declaracion_jurada":
        response_text = _generate_declaracion_jurada_response(query_lower, search_results)
    elif intent == "procedimiento":
        response_text = _generate_procedimiento_response(query_lower, search_results)
    elif intent == "diferencias":
        response_text = _generate_diferencias_response(query_lower, search_results)
    elif intent == "componentes":
        response_text = _generate_componentes_response(query_lower, search_results)
    elif intent == "viaticos_general":
        response_text = _generate_viaticos_response(query_lower, search_results)
    else:
        response_text = _generate_smart_fallback(request.message, search_results)
    
    # ---------------- LEGAL REASONING INTEGRATION ----------------
    if LEGAL_REASONING_AVAILABLE and legal_reasoner and search_results:
        try:
            legal_analysis = legal_reasoner.provide_legal_reasoning(request.message, search_results)
            response_text = f"{response_text}\n\n{legal_analysis}"
            logger.info("âœ… Legal reasoning applied to response")
        except Exception as e:
            logger.error(f"âŒ Error in legal reasoning: {e}")
    
    # Prepare sources from search results
    sources = []
    for i, result in enumerate(search_results[:3]):  # Top 3 sources
        source = {
            "title": result.get('title', f"Documento {i+1}"),
            "excerpt": result.get('content', '')[:200] + "...",
            "confidence": round(result.get('score', 0.0), 2),
            "method": result.get('method', 'hybrid')
        }
        sources.append(source)
    
    return {
        "response": response_text,
        "conversation_id": request.conversation_id or f"conv_{int(time.time())}",
        "timestamp": datetime.now().isoformat(),
        "sources": sources,
        "processing_time": round(processing_time, 3),
        "total_results": len(search_results)
    }

def _detect_query_intent(query: str) -> str:
    """
    Detecta la intenciÃ³n de la consulta usando patrones sofisticados
    """
    query_lower = query.lower()
    
    # Patrones para montos y lÃ­mites
    monto_patterns = [
        r'monto\s*(mÃ¡ximo|maximo|tope|lÃ­mite|limite)',
        r'(cuÃ¡nto|cuanto)\s*(es|son|puedo)',
        r'(mÃ¡ximo|maximo|tope)\s*(de|para)\s*viÃ¡ticos',
        r'lÃ­mite\s*(de|para)\s*viÃ¡ticos',
        r'tope\s*(de|para)\s*(viÃ¡ticos|viaticos|declaraciÃ³n)',
        r'(viÃ¡ticos|viaticos)\s*(mÃ¡ximos|maximos|diarios)',
        r'valor\s*(de|del)\s*viÃ¡tico'
    ]
    
    # Patrones para declaraciÃ³n jurada
    declaracion_patterns = [
        r'declaraciÃ³n\s*jurada',
        r'declaracion\s*jurada',
        r'sin\s*comprobante',
        r'sin\s*boleta',
        r'sin\s*factura',
        r'gastos\s*menores',
        r'lima|provincia'
    ]
    
    # Patrones para procedimientos
    procedimiento_patterns = [
        r'procedimiento',
        r'(cÃ³mo|como)\s*(solicitar|pedir|tramitar)',
        r'pasos\s*para',
        r'proceso\s*de',
        r'devoluciÃ³n',
        r'reembolso',
        r'devolver',
        r'gerente\s*asume'
    ]
    
    # Patrones para diferencias y comparaciones
    diferencia_patterns = [
        r'diferencia\s*entre',
        r'comparaciÃ³n',
        r'ministro.*servidor',
        r'servidor.*ministro',
        r'tipos\s*de\s*viÃ¡tico',
        r'categorÃ­as'
    ]
    
    # Patrones para componentes/quÃ© incluye
    componente_patterns = [
        r'(quÃ©|que)\s*incluye',
        r'componentes',
        r'(quÃ©|que)\s*cubre',
        r'alcance',
        r'comprende',
        r'abarca',
        r'incluyen\s*(los\s*)?viÃ¡ticos',
        r'incluyen\s*(los\s*)?viaticos'
    ]
    
    # Detectar intenciÃ³n con prioridad (orden importante)
    if any(re.search(pattern, query_lower) for pattern in componente_patterns):
        return "componentes"
    
    if any(re.search(pattern, query_lower) for pattern in procedimiento_patterns):
        return "procedimiento"
    
    if any(re.search(pattern, query_lower) for pattern in diferencia_patterns):
        return "diferencias"
    
    if any(re.search(pattern, query_lower) for pattern in monto_patterns):
        if any(re.search(pattern, query_lower) for pattern in declaracion_patterns):
            return "declaracion_jurada"
        return "montos_maximos"
    
    if "viÃ¡tico" in query_lower or "viatico" in query_lower:
        return "viaticos_general"
    
    return "general"

def _generate_viaticos_response(query: str, results: List[Dict]) -> str:
    """Generate specific response for viÃ¡ticos queries based on real documents."""
    if not results:
        return "No se encontraron documentos relevantes sobre viÃ¡ticos."
    
    query = query.lower()
    
    # Extraer contenido real de los documentos encontrados
    response_parts = ["ğŸ“‹ **INFORMACIÃ“N DE VIÃTICOS ENCONTRADA:**\n"]
    
    # Buscar montos especÃ­ficos en los documentos
    import re
    montos_encontrados = []
    contenido_relevante = []
    
    for i, result in enumerate(results[:3]):  # Usar top 3 resultados
        content = result.get('content', result.get('texto', ''))
        if content:
            # Buscar patrones de montos (S/ XXX.XX)
            montos = re.findall(r'S/\s*(\d+(?:\.\d{2})?)', content)
            for monto in montos:
                if monto not in montos_encontrados:
                    montos_encontrados.append(monto)
            
            # Extraer fragmento relevante
            fragment = content[:300] + "..." if len(content) > 300 else content
            contenido_relevante.append(f"**Fuente {i+1}:** {fragment}")
    
    # Mostrar montos encontrados
    if montos_encontrados:
        response_parts.append("ğŸ’° **MONTOS ENCONTRADOS:**")
        for monto in montos_encontrados[:5]:  # Mostrar hasta 5 montos
            response_parts.append(f"â€¢ S/ {monto} soles")
        response_parts.append("")
    
    # Mostrar contenido de documentos
    if contenido_relevante:
        response_parts.append("ğŸ“„ **CONTENIDO DE DOCUMENTOS:**")
        response_parts.extend(contenido_relevante)
        response_parts.append("")
    
    response_parts.append("ğŸ“„ **Base:** Documentos oficiales procesados del MINEDU")
    return "\n".join(response_parts)

def _generate_montos_maximos_response(query: str, results: List[Dict]) -> str:
    """Genera respuestas especÃ­ficas sobre montos mÃ¡ximos basada en documentos reales"""
    if not results:
        return "No se encontraron documentos sobre montos mÃ¡ximos de viÃ¡ticos."
    
    # Extraer informaciÃ³n real de los documentos
    response_parts = ["ğŸ“‹ **MONTOS MÃXIMOS ENCONTRADOS EN DOCUMENTOS:**\n"]
    
    import re
    montos_encontrados = []
    contenido_relevante = []
    
    for i, result in enumerate(results[:3]):
        content = result.get('content', result.get('texto', ''))
        if content:
            # Buscar patrones de montos
            montos = re.findall(r'S/\s*(\d+(?:\.\d{2})?)', content)
            for monto in montos:
                if monto not in montos_encontrados:
                    montos_encontrados.append(monto)
            
            # Extraer fragmento relevante
            fragment = content[:250] + "..." if len(content) > 250 else content
            contenido_relevante.append(f"**Documento {i+1}:** {fragment}")
    
    # Mostrar montos encontrados
    if montos_encontrados:
        response_parts.append("ğŸ’° **MONTOS IDENTIFICADOS:**")
        for monto in sorted(montos_encontrados, key=lambda x: float(x), reverse=True)[:5]:
            response_parts.append(f"â€¢ S/ {monto} soles")
        response_parts.append("")
    
    # Mostrar contenido de documentos
    if contenido_relevante:
        response_parts.append("ğŸ“„ **INFORMACIÃ“N DE DOCUMENTOS:**")
        response_parts.extend(contenido_relevante)
        response_parts.append("")
    
    response_parts.append("ğŸ“„ **Fuente:** Documentos oficiales del MINEDU")
    return "\n".join(response_parts)

def _generate_declaracion_jurada_response(query: str, results: List[Dict]) -> str:
    """Genera respuestas sobre declaraciÃ³n jurada"""
    # Detectar si pregunta por Lima o provincias especÃ­ficamente
    es_lima = "lima" in query
    es_provincia = "provincia" in query or "region" in query
    
    # Detectar si pregunta especÃ­ficamente sobre quÃ© cubre o limitaciones
    pregunta_limitado = "limitado" in query or "solo" in query or "movilidad" in query or "alimentaciÃ³n" in query
    
    if es_lima and not es_provincia:
        return """ğŸ“‹ **DECLARACIÃ“N JURADA - LIMA:**

ğŸ›ï¸ **MONTO MÃXIMO EN LIMA (Capital):**
â€¢ Hasta S/ [VALOR_TEMPORAL] soles por dÃ­a
â€¢ Para gastos menores sin comprobante
# TODO: Reemplazar con valor dinÃ¡mico real desde base de datos normativa

ğŸ“ **REQUISITOS:**
â€¢ Formato de declaraciÃ³n jurada institucional
â€¢ Firma del comisionado
â€¢ VÂ°BÂ° del jefe inmediato

âš ï¸ **APLICABLE SOLO PARA:**
â€¢ Movilidad local menor
â€¢ AlimentaciÃ³n sin comprobante
â€¢ Gastos urgentes e imprevistos"""
    
    elif es_provincia and not es_lima:
        if pregunta_limitado:
            logger.critical("PRODUCCIÃ“N: FunciÃ³n usa montos hardcodeados - reemplazar con consulta a base de datos")
            # TODO: Reemplazar con valor dinÃ¡mico real desde base de datos normativa
            return """ğŸ“‹ **DECLARACIÃ“N JURADA PROVINCIAS - ALCANCE:**

ğŸŒ„ **MONTO:** S/ [VALOR_TEMPORAL] soles por dÃ­a en provincias

âœ… **SÃ INCLUYE:**
â€¢ **Movilidad local:** Taxis, mototaxis, combis
â€¢ **AlimentaciÃ³n:** Restaurantes sin RUC, mercados, puestos
â€¢ **Propinas:** Razonables por servicios
â€¢ **Gastos menores:** Llamadas, fotocopias, etc.

âŒ **NO INCLUYE:**
â€¢ **Alojamiento:** SIEMPRE requiere factura/boleta
â€¢ **Pasajes:** Interprovinciales tienen otro presupuesto
â€¢ **Entretenimiento:** Cines, bares, etc.
â€¢ **Compras personales:** Ropa, souvenirs, etc.

ğŸ“Š **DISTRIBUCIÃ“N SUGERIDA:**
â€¢ Movilidad local: [PORCENTAJE_TEMPORAL] (40-50%)
â€¢ AlimentaciÃ³n: [PORCENTAJE_TEMPORAL] (35-40%)
â€¢ Gastos menores: [PORCENTAJE_TEMPORAL] (10-15%)
# TODO: Reemplazar con cÃ¡lculos dinÃ¡micos basados en monto real

âš ï¸ **LÃMITE:** MÃ¡ximo 30% del total de viÃ¡ticos asignados"""
        else:
            return """ğŸ“‹ **DECLARACIÃ“N JURADA - PROVINCIAS:**

ğŸŒ„ **MONTO MÃXIMO EN PROVINCIAS:**
â€¢ Hasta S/ [VALOR_TEMPORAL] soles por dÃ­a
â€¢ Para gastos menores sin comprobante
# TODO: Reemplazar con valor dinÃ¡mico real desde base de datos normativa

ğŸ“ **REQUISITOS:**
â€¢ Formato de declaraciÃ³n jurada institucional
â€¢ Detalle de gastos realizados
â€¢ AutorizaciÃ³n del supervisor

âš ï¸ **RESTRICCIONES:**
â€¢ No aplica para alojamiento
â€¢ Solo gastos menores de difÃ­cil sustentaciÃ³n"""
    
    else:
        return """ğŸ“‹ **DECLARACIÃ“N JURADA DE GASTOS - LÃMITES MINEDU:**

ğŸ›ï¸ **LIMA (Capital):**
â€¢ Hasta S/ [VALOR_TEMPORAL] soles por dÃ­a
â€¢ Aplicable dentro del Ã¡rea metropolitana

ğŸŒ„ **PROVINCIAS (Regiones):**
â€¢ Hasta S/ [VALOR_TEMPORAL] soles por dÃ­a
â€¢ Aplicable fuera de Lima Metropolitana
# TODO: Reemplazar con valores dinÃ¡micos reales desde base de datos normativa

ğŸ“ **CONDICIONES DE USO:**
â€¢ Solo para gastos menores sin comprobante de pago
â€¢ Debe detallarse el concepto del gasto
â€¢ Requiere formato institucional firmado
â€¢ Sujeto a verificaciÃ³n posterior

ğŸ’¡ **GASTOS TÃPICOS CUBIERTOS:**
â€¢ Movilidad local en taxi o mototaxi
â€¢ AlimentaciÃ³n en lugares sin RUC
â€¢ Propinas y gastos menores
â€¢ Peajes sin comprobante

âš ï¸ **NO APLICA PARA:**
â€¢ Alojamiento (requiere factura)
â€¢ Pasajes interprovinciales
â€¢ Gastos con comprobante disponible

ğŸ“– **REFERENCIA:** Numeral 8.4.17 - Directiva de ViÃ¡ticos MINEDU"""

def _generate_procedimiento_response(query: str, results: List[Dict]) -> str:
    """Genera respuestas sobre procedimientos"""
    if "formato" in query and ("fo-viat" in query.lower() or "viat-01" in query.lower()):
        return """ğŸ“‹ **FORMATO FO-VIAT-01 - SOLICITUD DE VIÃTICOS:**

ğŸ“ **Â¿QUÃ‰ ES?**
â€¢ Formato institucional para solicitar viÃ¡ticos
â€¢ Documento oficial del MINEDU
â€¢ Requerido para TODAS las comisiones de servicio

ğŸ“Š **CONTENIDO DEL FORMATO:**
â€¢ Datos personales del solicitante
â€¢ Destino y fechas del viaje
â€¢ Motivo/objetivo de la comisiÃ³n
â€¢ Actividades a realizar
â€¢ Presupuesto estimado
â€¢ Medio de transporte

âœ… **DÃ“NDE OBTENERLO:**
â€¢ Oficina de AdministraciÃ³n de tu dependencia
â€¢ Sistema interno MINEDU (digital)
â€¢ Ãrea de Recursos Humanos
â€¢ Portal web institucional

â° **CUÃNDO PRESENTARLO:**
â€¢ MÃ­nimo 5 dÃ­as hÃ¡biles antes del viaje
â€¢ AcompaÃ±ado del plan de trabajo
â€¢ Con VÂ°BÂ° del jefe inmediato

ğŸ“‹ **DOCUMENTOS ADJUNTOS:**
â€¢ Plan de trabajo detallado
â€¢ InvitaciÃ³n o convocatoria (si aplica)
â€¢ Agenda de actividades
â€¢ Presupuesto de gastos estimado

âš ï¸ **IMPORTANTE:** Sin este formato NO se autoriza ningÃºn viÃ¡tico"""
    elif "devoluciÃ³n" in query or "devolver" in query or "gerente" in query:
        return """ğŸ“‹ **PROCEDIMIENTO DE DEVOLUCIÃ“N DE VIÃTICOS:**

ğŸ”„ **CUANDO EL GERENTE/DIRECTOR ASUME EL GASTO:**

1ï¸âƒ£ **SITUACIÃ“N APLICABLE:**
â€¢ Viaje cancelado o postergado
â€¢ ComisiÃ³n cumplida en menor tiempo
â€¢ Gastos asumidos por la entidad anfitriona

2ï¸âƒ£ **PASOS DEL PROCEDIMIENTO:**
â€¢ Comunicar inmediatamente la situaciÃ³n al Ã¡rea administrativa
â€¢ Llenar formato de devoluciÃ³n (FO-VIAT-03)
â€¢ Adjuntar documentaciÃ³n sustentatoria
â€¢ Calcular monto a devolver proporcionalmente

3ï¸âƒ£ **PLAZOS:**
â€¢ MÃ¡ximo 3 dÃ­as hÃ¡biles despuÃ©s del retorno
â€¢ DevoluciÃ³n efectiva en 48 horas

4ï¸âƒ£ **FORMAS DE DEVOLUCIÃ“N:**
â€¢ DepÃ³sito en cuenta institucional
â€¢ Descuento en planilla (previa autorizaciÃ³n)
â€¢ Efectivo en caja central

âš ï¸ **IMPORTANTE:**
â€¢ No devolver implica falta administrativa
â€¢ Se emite constancia de devoluciÃ³n
â€¢ Afecta futuras asignaciones de viÃ¡ticos"""
    else:
        return """ğŸ“‹ **PROCEDIMIENTO GENERAL PARA SOLICITUD DE VIÃTICOS:**

ğŸ“ **PASO 1: SOLICITUD**
â€¢ Llenar formato FO-VIAT-01
â€¢ MÃ­nimo 5 dÃ­as hÃ¡biles de anticipaciÃ³n
â€¢ Adjuntar plan de trabajo y agenda

âœ… **PASO 2: APROBACIÃ“N**
â€¢ VÂ°BÂ° del jefe inmediato
â€¢ RevisiÃ³n de disponibilidad presupuestal
â€¢ AutorizaciÃ³n de la Oficina de AdministraciÃ³n

ğŸ’° **PASO 3: ASIGNACIÃ“N**
â€¢ CÃ¡lculo segÃºn escala vigente
â€¢ DepÃ³sito en cuenta o entrega en efectivo
â€¢ Entrega 24 horas antes del viaje

ğŸ“Š **PASO 4: RENDICIÃ“N**
â€¢ Plazo mÃ¡ximo: 10 dÃ­as hÃ¡biles post-viaje
â€¢ Comprobantes originales
â€¢ Informe de comisiÃ³n de servicios

ğŸ” **PASO 5: LIQUIDACIÃ“N**
â€¢ RevisiÃ³n de documentos
â€¢ DevoluciÃ³n de excedentes (si aplica)
â€¢ Archivo en legajo personal"""
    
    if "resoluciÃ³n" in query or "resolucion" in query:
        return """ğŸ“‹ **RESOLUCIÃ“N DE ADMINISTRACIÃ“N PARA VIÃTICOS:**

ğŸ“„ **Â¿SE NECESITA RESOLUCIÃ“N?**
â€¢ **SÃ** para viajes fuera de Lima Metropolitana
â€¢ **SÃ** para comisiones de servicio oficiales
â€¢ **SÃ** para viajes al extranjero
â€¢ **NO** para viajes locales dentro de Lima

ğŸ“ **CONTENIDO DE LA RESOLUCIÃ“N:**
â€¢ NÃºmero de resoluciÃ³n y fecha
â€¢ Nombre y cargo del comisionado
â€¢ Destino especÃ­fico del viaje
â€¢ Fechas de inicio y fin
â€¢ Motivo de la comisiÃ³n
â€¢ Monto autorizado por dÃ­a
â€¢ Fuente de financiamiento

âš ï¸ **PARA REEMBOLSOS:**
â€¢ **NO se requiere resoluciÃ³n adicional**
â€¢ Se usa la resoluciÃ³n de autorizaciÃ³n original
â€¢ Solo se adjunta liquidaciÃ³n de gastos
â€¢ Comprobantes y rendiciÃ³n de cuentas

ğŸ”„ **PROCESO DE EMISIÃ“N:**
â€¢ AdministraciÃ³n evalÃºa la solicitud
â€¢ Verifica disponibilidad presupuestal
â€¢ Emite resoluciÃ³n en 2-3 dÃ­as hÃ¡biles
â€¢ Se notifica al solicitante

ğŸ“– **BASE LEGAL:** Directiva de ViÃ¡ticos MINEDU - Numeral 8.2.5"""

def _generate_diferencias_response(query: str, results: List[Dict]) -> str:
    """Genera respuestas sobre diferencias entre categorÃ­as"""
    logger.critical("PRODUCCIÃ“N: FunciÃ³n _generate_diferencias_response usa montos hardcodeados - reemplazar con consulta a base de datos")
    # TODO: Reemplazar con valores dinÃ¡micos reales desde base de datos normativa
    return """ğŸ“‹ **DIFERENCIAS DE VIÃTICOS POR CATEGORÃA:**

ğŸ‘‘ **MINISTROS Y VICEMINISTROS**
â€¢ Monto diario: S/ [VALOR_TEMPORAL]
â€¢ Sin lÃ­mite de dÃ­as al mes
â€¢ Pasajes en clase ejecutiva (vuelos > 5 horas)
â€¢ Alojamiento categorÃ­a 4-5 estrellas

ğŸ›ï¸ **FUNCIONARIOS Y DIRECTIVOS**
â€¢ Monto diario: S/ [VALOR_TEMPORAL]
â€¢ MÃ¡ximo 15 dÃ­as al mes
â€¢ Pasajes en clase econÃ³mica
â€¢ Alojamiento categorÃ­a 3-4 estrellas

ğŸ‘¥ **SERVIDORES CIVILES**
â€¢ Monto diario: S/ [VALOR_TEMPORAL]
â€¢ MÃ¡ximo 10 dÃ­as al mes
â€¢ Pasajes en clase econÃ³mica
â€¢ Alojamiento categorÃ­a 2-3 estrellas

ğŸ“Š **CUADRO COMPARATIVO:**
```
CategorÃ­a          | Monto/dÃ­a        | DÃ­as/mes  | Clase viaje
-------------------|------------------|-----------|-------------
Ministros          | S/ [TEMPORAL]    | Sin lÃ­mite| Ejecutiva*
Funcionarios       | S/ [TEMPORAL]    | 15 dÃ­as   | EconÃ³mica
Servidores         | S/ [TEMPORAL]    | 10 dÃ­as   | EconÃ³mica
```
# TODO: Reemplazar tabla con valores dinÃ¡micos reales desde base de datos normativa
*En vuelos mayores a 5 horas

âš ï¸ **NOTA:** Los montos son uniformes para territorio nacional. Las diferencias principales estÃ¡n en lÃ­mites de dÃ­as y condiciones de viaje."""

def _generate_componentes_response(query: str, results: List[Dict]) -> str:
    """Genera respuestas sobre quÃ© incluyen los viÃ¡ticos"""
    return """ğŸ“‹ **Â¿QUÃ‰ INCLUYEN LOS VIÃTICOS DIARIOS?**

Los viÃ¡ticos cubren TRES componentes principales:

ğŸ¨ **1. ALOJAMIENTO (40-50% del monto)**
â€¢ Hospedaje en hotel o similar
â€¢ Incluye desayuno si estÃ¡ disponible
â€¢ Debe sustentarse con factura

ğŸ½ï¸ **2. ALIMENTACIÃ“N (30-40% del monto)**
â€¢ Desayuno (si no incluye el hotel)
â€¢ Almuerzo
â€¢ Cena
â€¢ Refrigerios necesarios

ğŸš— **3. MOVILIDAD LOCAL (10-20% del monto)**
â€¢ Traslados aeropuerto/terminal - hotel
â€¢ Desplazamientos para la comisiÃ³n
â€¢ NO incluye pasajes interprovinciales

ğŸ’¡ **GASTOS ADICIONALES CUBIERTOS:**
â€¢ Propinas razonables
â€¢ Llamadas telefÃ³nicas de servicio
â€¢ Internet (si es necesario para la comisiÃ³n)
â€¢ LavanderÃ­a (comisiones > 7 dÃ­as)

âŒ **NO ESTÃN INCLUIDOS:**
â€¢ Gastos personales
â€¢ Bebidas alcohÃ³licas
â€¢ Entretenimiento
â€¢ Compras personales
â€¢ Multas o penalidades
â€¢ Gastos mÃ©dicos (usar seguro)

ğŸ“Š **DISTRIBUCIÃ“N REFERENCIAL:**
Para S/ 320 diarios:
â€¢ Alojamiento: S/ 130-160
â€¢ AlimentaciÃ³n: S/ 95-130
â€¢ Movilidad: S/ 30-65"""

def _generate_smart_fallback(query: str, results: List[Dict]) -> str:
    """Genera un fallback inteligente cuando no se detecta una categorÃ­a especÃ­fica"""
    return f"""ğŸ“‹ **SISTEMA DE CONSULTAS MINEDU:**

No encontrÃ© informaciÃ³n especÃ­fica sobre: "{query}"

Sin embargo, puedo ayudarte con estos temas relacionados:

ğŸ“Œ **TEMAS DISPONIBLES:**
â€¢ ğŸ’° Montos mÃ¡ximos de viÃ¡ticos por dÃ­a
â€¢ ğŸ“ LÃ­mites de declaraciÃ³n jurada (Lima/Provincias)  
â€¢ ğŸ”„ Procedimientos de solicitud y devoluciÃ³n
â€¢ ğŸ“Š Diferencias entre categorÃ­as de personal
â€¢ ğŸ¯ QuÃ© incluyen los viÃ¡ticos diarios
â€¢ ğŸ“… Plazos y requisitos de rendiciÃ³n

ğŸ’¡ **SUGERENCIAS DE CONSULTA:**
â€¢ "Â¿CuÃ¡l es el monto mÃ¡ximo de viÃ¡ticos?"
â€¢ "Â¿CÃ³mo solicito viÃ¡ticos para un viaje?"
â€¢ "Â¿QuÃ© diferencia hay entre viÃ¡ticos de ministros y servidores?"
â€¢ "Â¿QuÃ© gastos cubre el viÃ¡tico diario?"

ğŸ” **REFORMULA TU PREGUNTA:**
Intenta ser mÃ¡s especÃ­fico o usa palabras clave como: viÃ¡ticos, montos, declaraciÃ³n jurada, procedimiento, etc."""

def _generate_general_response(query: str, results: List[Dict]) -> str:
    """Generate general response based on search results."""
    if results:
        content = results[0].get('content', '')
        return f"ğŸ“‹ **INFORMACIÃ“N ENCONTRADA:**\n\n{content[:300]}...\n\nğŸ“„ **FUENTE:** Documentos oficiales MINEDU"
    else:
        return "ğŸ“‹ He buscado en los documentos disponibles pero no encontrÃ© informaciÃ³n especÃ­fica sobre tu consulta. Â¿PodrÃ­as ser mÃ¡s especÃ­fico?"

def _generate_fallback_response(request: ChatRequest, error_msg: str = None) -> Dict:
    """Generate fallback response when hybrid search is not available."""
    logger.critical("PRODUCCIÃ“N: FunciÃ³n _generate_fallback_response usa montos hardcodeados - reemplazar con consulta a base de datos")
    # TODO: Reemplazar con valores dinÃ¡micos reales desde base de datos normativa
    response_text = """ğŸ“‹ **SISTEMA EN MODO BÃSICO**

âš ï¸ El sistema de bÃºsqueda avanzada no estÃ¡ disponible temporalmente.

ğŸ’¡ **INFORMACIÃ“N GENERAL DE VIÃTICOS MINEDU:**
â€¢ Ministros de Estado: S/ [VALOR_TEMPORAL] diarios
â€¢ Servidores Civiles: S/ [VALOR_TEMPORAL] diarios
â€¢ DeclaraciÃ³n Jurada: LÃ­mites especÃ­ficos segÃºn territorio

ğŸ”§ **ESTADO:** Funcionando con informaciÃ³n bÃ¡sica"""
    
    if error_msg:
        response_text += f"\n\nğŸ” **Detalle tÃ©cnico:** {error_msg}"
    
    return {
        "response": response_text,
        "conversation_id": request.conversation_id or f"conv_{int(time.time())}",
        "timestamp": datetime.now().isoformat(),
        "sources": [{"title": "Sistema bÃ¡sico", "excerpt": "InformaciÃ³n general disponible", "confidence": 0.5}],
        "processing_time": 0.1,
        "mode": "fallback"
    }

# Simple test endpoint
@app.post("/api/chat/test")
async def chat_test():
    """Ultra simple test endpoint."""
    return {"message": "Chat test successful!", "timestamp": datetime.now().isoformat()}

@app.post("/api/search")
async def search_endpoint(request: SearchRequest):
    """Search endpoint for document retrieval."""
    try:
        # TODO: Reemplazar con procesamiento real sin simulaciÃ³n
        logger.critical("PRODUCCIÃ“N: Endpoint /api/search usa simulaciÃ³n - reemplazar con bÃºsqueda real")
        import asyncio
        # await asyncio.sleep(0.3)  # REMOVIDO: No simular tiempo en producciÃ³n
        
        # TODO CRITICAL: Reemplazar con hybrid search real - NO usar mock en producciÃ³n
        mock_results = {
            "query": request.query,
            "method": request.method,
            "total_results": 3,
            "processing_time": 0.3,
            "results": [
                {
                    "id": 1,
                    "title": "Resultado de bÃºsqueda 1",
                    "content": f"Este documento contiene informaciÃ³n relevante sobre '{request.query}'. "
                              f"AquÃ­ encontrarÃ¡s detalles especÃ­ficos sobre el tema consultado.",
                    "score": 0.95,
                    "source": "documento_1.pdf",
                    "page": 1
                },
                {
                    "id": 2,
                    "title": "Resultado de bÃºsqueda 2",
                    "content": f"InformaciÃ³n adicional relacionada con '{request.query}'. "
                              f"Este contenido complementa la consulta realizada.",
                    "score": 0.87,
                    "source": "documento_2.pdf", 
                    "page": 3
                },
                {
                    "id": 3,
                    "title": "Resultado de bÃºsqueda 3",
                    "content": f"Datos relevantes sobre '{request.query}'. "
                              f"AquÃ­ se incluyen aspectos tÃ©cnicos del tema.",
                    "score": 0.79,
                    "source": "documento_3.pdf",
                    "page": 2
                }
            ]
        }
        
        return mock_results
        
    except Exception as e:
        logger.error(f"Error in search endpoint: {e}")
        raise HTTPException(status_code=500, detail="Error processing search request")

# ==================== NUEVOS ENDPOINTS LANGCHAIN ====================

@app.post("/api/chat/professional")
async def chat_langgraph_professional(request: ChatRequest):
    """Endpoint PROFESIONAL - Con RAG real usando SimpleRetriever + Legal Reasoning"""
    try:
        # PARCHE: Usar SimpleRetriever si LangGraph no estÃ¡ disponible
        if not LANGGRAPH_PROFESSIONAL_AVAILABLE and retriever:
            logger.info(f"ğŸ”§ PARCHE: Usando SimpleRetriever para consulta profesional: {request.message}")
            start_time = time.time()
            
            # BÃºsqueda con SimpleRetriever
            documents = retriever.simple_similarity_search(request.message, k=5)
            processing_time = time.time() - start_time
            
            if documents:
                # Generar respuesta profesional basada en documentos reales
                response_parts = [
                    "ğŸ“‹ **SISTEMA PROFESIONAL RAG MINEDU**",
                    f"ğŸ” **Consulta**: {request.message}",
                    f"ğŸ“Š **Documentos analizados**: {len(documents)}",
                    "",
                    "ğŸ“„ **RESULTADOS BASADOS EN NORMATIVA REAL**:"
                ]
                
                sources = []
                for i, doc in enumerate(documents[:3], 1):
                    content_preview = doc.page_content[:300].replace('\n', ' ')
                    response_parts.append(f"**{i}.** {content_preview}...")
                    
                    sources.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "title": doc.metadata.get("titulo", f"Documento {i}"),
                        "confidence": 0.85,
                        "source_id": doc.metadata.get("id", f"doc_{i}")
                    })
                
                # Integrar razonamiento legal
                if LEGAL_REASONING_AVAILABLE and legal_reasoner:
                    try:
                        legal_analysis = legal_reasoner.provide_legal_reasoning(
                            request.message,
                            [{"content": doc.page_content, "title": doc.metadata.get("titulo", "")} for doc in documents]
                        )
                        response_parts.append("")
                        response_parts.append(legal_analysis)
                    except Exception as le:
                        logger.error(f"Error in legal reasoning: {le}")
                
                response_text = "\n".join(response_parts)
                
                return {
                    "response": response_text,
                    "sources": sources,
                    "intent": "consulta_profesional",
                    "intent_entities": {"documents_found": len(documents)},
                    "agent_used": "simple_retriever_professional",
                    "documents_found": len(documents),
                    "confidence": 0.85,
                    "processing_time": round(processing_time, 3),
                    "method": "simple_retriever_professional",
                    "success": True,
                    "timestamp": datetime.now().isoformat(),
                    "system_info": {"retriever": "SimpleRetriever", "legal_reasoning": LEGAL_REASONING_AVAILABLE},
                    "thread_id": request.conversation_id or f"prof_{int(time.time())}",
                    "system": "RAG_REAL_PROFESSIONAL"
                }
            else:
                return {
                    "response": "ğŸ“‹ **SISTEMA PROFESIONAL RAG MINEDU**\n\nâŒ No se encontraron documentos relevantes para la consulta.",
                    "sources": [],
                    "documents_found": 0,
                    "confidence": 0.0,
                    "processing_time": round(processing_time, 3),
                    "method": "simple_retriever_professional_no_results",
                    "success": False,
                    "timestamp": datetime.now().isoformat()
                }
        
        # Si LangGraph estÃ¡ disponible, usarlo
        elif LANGGRAPH_PROFESSIONAL_AVAILABLE:
            logger.info("ğŸš€ Usando LangGraph PROFESIONAL original")
            logger.info(f"ğŸš€ Testing LangGraph PROFESIONAL con query: {request.message}")
            start_time = time.time()
            
            # Procesar con LangGraph PROFESIONAL
            thread_id = request.conversation_id or f"prof_{int(time.time())}"
            result = await professional_orchestrator.process_query_professional(request.message, thread_id=thread_id)
            processing_time = time.time() - start_time
            
            # ---- IntegraciÃ³n de Razonamiento Legal ----
            combined_response = result.get("response", "")
            if LEGAL_REASONING_AVAILABLE and legal_reasoner and result.get("sources"):
                try:
                    legal_analysis = legal_reasoner.provide_legal_reasoning(request.message, result.get("sources", []))
                    combined_response = f"{combined_response}\n\n{legal_analysis}"
                except Exception as le:
                    logger.error(f"Error integrating legal reasoning: {le}")
            
            return {
                "response": combined_response,
                "sources": result.get("sources", []),
                "intent": result.get("intent", ""),
                "intent_entities": result.get("intent_entities", {}),
                "agent_used": result.get("agent_used", ""),
                "documents_found": result.get("documents_found", 0),
                "confidence": result.get("confidence", 0.0),
                "processing_time": round(processing_time, 3),
                "method": "langgraph_professional_direct",
                "success": not bool(result.get("error")),
                "timestamp": datetime.now().isoformat(),
                "system_info": result.get("orchestrator_info", {}),
                "thread_id": thread_id,
                "system": "LANGGRAPH_PROFESSIONAL"
            }
        
        # Caso cuando ni LangGraph ni retriever estÃ¡n disponibles
        else:
            raise HTTPException(
                status_code=503, 
                detail="Sistema profesional no disponible. Ni LangGraph ni SimpleRetriever estÃ¡n operativos."
            )
        
    except Exception as e:
        logger.error(f"âŒ Error en endpoint LangGraph PROFESIONAL: {e}")
        raise HTTPException(status_code=500, detail=f"Error en LangGraph PROFESIONAL: {str(e)}")

@app.post("/api/chat/langgraph")
async def chat_langgraph_real(request: ChatRequest):
    """Endpoint directo para testing LangGraph REAL - StateGraph + CompiledGraph"""
    try:
        if not LANGGRAPH_REAL_AVAILABLE:
            raise HTTPException(
                status_code=503, 
                detail="LangGraph REAL no disponible. Verifica la instalaciÃ³n."
            )
        
        logger.info(f"ğŸš€ Testing LangGraph REAL con query: {request.message}")
        start_time = time.time()
        
        # Procesar directamente con LangGraph REAL
        thread_id = request.conversation_id or f"test_{int(time.time())}"
        result = await real_orchestrator.process_query_real(request.message, thread_id=thread_id)
        processing_time = time.time() - start_time
        
        return {
            "response": result.get("response", ""),
            "sources": result.get("sources", []),
            "intent": result.get("intent", ""),
            "agent_used": result.get("agent_used", ""),
            "documents_found": result.get("documents_found", 0),
            "confidence": result.get("confidence", 0.0),
            "processing_time": round(processing_time, 3),
            "method": "langgraph_real_direct",
            "success": not bool(result.get("error")),
            "timestamp": datetime.now().isoformat(),
            "system_info": result.get("orchestrator_info", {}),
            "extracted_info": result.get("extracted_info", {}),
            "thread_id": thread_id,
            "langgraph_version": "real"
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en endpoint LangGraph REAL: {e}")
        raise HTTPException(status_code=500, detail=f"Error en LangGraph REAL: {str(e)}")

@app.post("/api/chat/langchain")
async def chat_langchain_real(request: ChatRequest):
    """
    Endpoint LangChain REAL - Sistema Antialucinaciones v2.0.0
    PROHIBIDO: Simular, inventar o generar respuestas falsas
    OBLIGATORIO: Solo procesar con datos reales y fuentes verificables
    """
    try:
        # ValidaciÃ³n de entrada gubernamental
        if not request.message or len(request.message.strip()) == 0:
            logger.error("âŒ CRÃTICO: Query vacÃ­o en sistema gubernamental")
            raise HTTPException(
                status_code=400, 
                detail="Query requerido para sistema gubernamental"
            )
        
        logger.info(f"ğŸ›ï¸ Procesando query gubernamental REAL: {request.message}")
        start_time = time.time()
        
        # SOLO procesamiento real - PROHIBIDO simular
        if not LANGGRAPH_REAL_AVAILABLE or not real_orchestrator:
            logger.error("âŒ CRÃTICO: Sistema real no disponible - NO SE SIMULA")
            raise HTTPException(
                status_code=503, 
                detail="Sistema real LangGraph no disponible. PROHIBIDO simular en entorno gubernamental."
            )
        
        # Procesar SOLO con sistema real
        result = await real_orchestrator.process_query(request.message)
        processing_time = time.time() - start_time
        
        # ValidaciÃ³n de autenticidad de resultados
        if not result or result.get("response", "").strip() == "":
            logger.warning("âš ï¸ Resultado vacÃ­o - sistema falla seguro")
            return {
                "response": "",
                "sources": [],
                "intent": "",
                "agent_used": "real_langgraph",
                "documents_found": 0,
                "confidence": 0.0,
                "processing_time": round(processing_time, 3),
                "method": "langgraph_real_only",
                "success": False,
                "timestamp": datetime.now().isoformat(),
                "system_info": {"status": "fail_safe_empty_result"},
                "extracted_info": {},
                "langgraph_version": "real_v2.0.0",
                "government_compliance": True,
                "anti_hallucination": True
            }
        
        return {
            "response": result.get("response", ""),
            "sources": result.get("sources", []),
            "intent": result.get("intent", ""),
            "agent_used": "real_langgraph",
            "documents_found": result.get("documents_found", 0),
            "confidence": result.get("confidence", 0.0),
            "processing_time": round(processing_time, 3),
            "method": "langgraph_real_only",
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "system_info": result.get("orchestrator_info", {}),
            "extracted_info": result.get("extracted_info", {}),
            "langgraph_version": "real_v2.0.0",
            "government_compliance": True,
            "anti_hallucination": True
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en endpoint LangChain REAL: {e}")
        raise HTTPException(status_code=500, detail=f"Error en sistema real: {str(e)}")

@app.get("/api/admin/langchain/status")
async def langchain_system_status():
    """Estado del sistema LangGraph REAL y simuladores"""
    try:
        status_info = {
            "timestamp": datetime.now().isoformat()
        }
        
        # LangGraph REAL status (PRINCIPAL)
        if LANGGRAPH_REAL_AVAILABLE and real_orchestrator:
            real_status = real_orchestrator.get_system_status()
            status_info.update({
                "status": "operational",
                "langgraph_real_available": True,
                "langgraph_real_status": real_status,
                "primary_system": "langgraph_real"
            })
        else:
            status_info.update({
                "langgraph_real_available": False,
                "langgraph_real_error": "LangGraph REAL not loaded"
            })
        
        # Sistema Antialucinaciones v2.0.0 - Solo sistemas reales
        status_info.update({
            "anti_hallucination_system": "v2.0.0",
            "government_compliance": True,
            "simulation_disabled": True,
            "real_only_processing": True,
            "langchain_simulator_available": False  # Permanentemente deshabilitado
            })
        
        # Hybrid fallback
        status_info["hybrid_fallback_available"] = HYBRID_SEARCH_AVAILABLE
        
        # Config
        status_info["config"] = langchain_config.to_dict()
        
        # Determinar status global
        if status_info.get("langgraph_real_available"):
            status_info["status"] = "operational"
        elif status_info.get("langchain_simulator_available"):
            status_info["status"] = "simulator_only"
        else:
            status_info["status"] = "unavailable"
            status_info["error"] = "No hay sistemas LangGraph/LangChain disponibles"
        
        return status_info
        
    except Exception as e:
        logger.error(f"Error obteniendo estado LangChain: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/admin/documents/stats")
async def document_statistics():
    """EstadÃ­sticas de documentos cargados"""
    try:
        if not LANGCHAIN_AVAILABLE:
            raise HTTPException(status_code=503, detail="LangChain no disponible")
        
        stats = retriever.get_stats()
        
        return {
            "status": "success",
            "statistics": stats,
            "chunks_file": retriever.chunks_path,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo estadÃ­sticas: {e}")
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/api/admin/test-queries")
async def test_critical_queries():
    """Test de queries crÃ­ticas para validar migraciÃ³n"""
    
    critical_queries = [
        "Â¿CuÃ¡l es el monto mÃ¡ximo de viÃ¡ticos en provincias?",
        "Â¿CuÃ¡nto es el lÃ­mite de declaraciÃ³n jurada en Lima?", 
        "Â¿CuÃ¡l es el monto de viÃ¡ticos para servidores civiles?",
        "Â¿QuÃ© documentos requieren los viÃ¡ticos?",
        "Â¿CuÃ¡l es el procedimiento para solicitar viÃ¡ticos?"
    ]
    
    results = []
    
    for query in critical_queries:
        try:
            if LANGCHAIN_AVAILABLE:
                # Test con LangChain
                start_time = time.time()
                langchain_result = orchestrator.process_query(query)
                langchain_time = time.time() - start_time
                
                results.append({
                    "query": query,
                    "langchain": {
                        "success": not bool(langchain_result.get("error")),
                        "response_length": len(langchain_result.get("response", "")),
                        "sources_found": len(langchain_result.get("sources", [])),
                        "processing_time": round(langchain_time, 3),
                        "intent": langchain_result.get("intent", ""),
                        "confidence": langchain_result.get("confidence", 0.0)
                    }
                })
            else:
                results.append({
                    "query": query,
                    "langchain": {"error": "LangChain no disponible"}
                })
                
        except Exception as e:
            results.append({
                "query": query,
                "error": str(e)
            })
    
    # Calcular estadÃ­sticas
    successful_queries = sum(1 for r in results if r.get("langchain", {}).get("success", False))
    total_queries = len(results)
    
    return {
        "test_results": results,
        "summary": {
            "total_queries": total_queries,
            "successful": successful_queries,
            "success_rate": f"{(successful_queries/total_queries)*100:.1f}%" if total_queries > 0 else "0%",
            "langchain_available": LANGCHAIN_AVAILABLE,
            "timestamp": datetime.now().isoformat()
        }
    }

@app.post("/api/admin/compare-systems")
async def compare_langchain_vs_legacy(request: ChatRequest):
    """Comparar respuesta LangChain vs sistema legacy para debugging"""
    try:
        comparison_result = {
            "query": request.message,
            "timestamp": datetime.now().isoformat()
        }
        
        # Test con LangChain si estÃ¡ disponible
        if LANGCHAIN_AVAILABLE:
            try:
                start_time = time.time()
                langchain_result = orchestrator.process_query(request.message)
                langchain_time = time.time() - start_time
                
                comparison_result["langchain"] = {
                    "success": not bool(langchain_result.get("error")),
                    "response": langchain_result.get("response", ""),
                    "sources": langchain_result.get("sources", []),
                    "processing_time": round(langchain_time, 3),
                    "confidence": langchain_result.get("confidence", 0.0),
                    "intent": langchain_result.get("intent", ""),
                    "documents_found": langchain_result.get("documents_found", 0)
                }
            except Exception as e:
                comparison_result["langchain"] = {"error": str(e)}
        else:
            comparison_result["langchain"] = {"error": "No disponible"}
        
        # Test con sistema legacy si estÃ¡ disponible
        if HYBRID_SEARCH_AVAILABLE:
            try:
                # SimulaciÃ³n de sistema legacy (sin ejecutar bÃºsqueda real para evitar timeouts)
                comparison_result["legacy"] = {
                    "method": "hybrid_legacy",
                    "note": "Sistema hÃ­brido legacy disponible como fallback"
                }
            except Exception as e:
                comparison_result["legacy"] = {"error": str(e)}
        else:
            comparison_result["legacy"] = {"error": "No disponible"}
        
        return comparison_result
        
    except Exception as e:
        logger.error(f"Error en comparaciÃ³n de sistemas: {e}")
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

# ============================================================================
# ENTERPRISE ENDPOINTS: Feedback System & Advanced Reranking
# ============================================================================

if ENTERPRISE_FEATURES_AVAILABLE:
    
    @app.post("/api/feedback")
    async def submit_feedback_endpoint(
        feedback_request: FeedbackRequest,
        current_user: User = Depends(get_current_user),
        session = Depends(get_async_session)
    ):
        """Submit user feedback for query improvement"""
        try:
            result = await submit_user_feedback(current_user.id, feedback_request, session)
            logger.info(f"âœ… Feedback submitted by user {current_user.id} for query {feedback_request.query_log_id}")
            return result
        except Exception as e:
            logger.error(f"âŒ Error submitting feedback: {e}")
            raise HTTPException(status_code=500, detail=f"Error submitting feedback: {str(e)}")
    
    @app.get("/api/feedback/analytics")
    async def get_feedback_analytics_endpoint(
        days: int = 30,
        current_user: User = Depends(get_current_user),
        session = Depends(get_async_session)
    ):
        """Get feedback analytics (admin only)"""
        try:
            if current_user.role != "admin":
                raise HTTPException(status_code=403, detail="Admin access required")
            
            analytics = await get_system_feedback_analytics(session, days)
            logger.info(f"ğŸ“Š Feedback analytics requested by admin {current_user.id}")
            return analytics
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"âŒ Error getting feedback analytics: {e}")
            raise HTTPException(status_code=500, detail=f"Error getting analytics: {str(e)}")
    
    @app.get("/api/reranker/status")
    async def get_reranker_status_endpoint(
        current_user: User = Depends(get_current_user)
    ):
        """Get advanced reranker status"""
        try:
            from .core.reranking.advanced_reranker import get_reranker_status
            status = get_reranker_status()
            logger.info(f"ğŸ”„ Reranker status requested by user {current_user.id}")
            return {
                "reranker_status": status,
                "user_id": str(current_user.id),
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"âŒ Error getting reranker status: {e}")
            raise HTTPException(status_code=500, detail=f"Error getting reranker status: {str(e)}")
    
    @app.post("/api/reranker/initialize")
    async def initialize_reranker_endpoint(
        current_user: User = Depends(get_current_user)
    ):
        """Initialize advanced reranker (admin only)"""
        try:
            if current_user.role != "admin":
                raise HTTPException(status_code=403, detail="Admin access required")
            
            from .core.reranking.advanced_reranker import initialize_reranker
            success = await initialize_reranker()
            
            logger.info(f"ğŸš€ Reranker initialization {'successful' if success else 'failed'} by admin {current_user.id}")
            return {
                "success": success,
                "message": "Reranker initialized successfully" if success else "Reranker initialization failed",
                "cross_encoder_available": success,
                "timestamp": datetime.now().isoformat()
            }
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"âŒ Error initializing reranker: {e}")
            raise HTTPException(status_code=500, detail=f"Error initializing reranker: {str(e)}")

else:
    logger.warning("âš ï¸ Enterprise endpoints disabled - Auth system not available")

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8001,  # Use different port to avoid conflict
        reload=True,
        log_level="info"
    )