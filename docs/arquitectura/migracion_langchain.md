# üèóÔ∏è Arquitectura de Migraci√≥n LangChain/LangGraph

> **Documento**: Especificaci√≥n t√©cnica de migraci√≥n  
> **Fecha**: 2025-07-01  
> **Estado**: Planificado - Pendiente implementaci√≥n  
> **Objetivo**: Migrar de sistema hardcodeado a RAG real  

## üìä DIAGN√ìSTICO DEL PROBLEMA ACTUAL

### ‚ùå **Sistema con Respuestas Hardcodeadas**
```python
# backend/src/main.py - C√≥digo problem√°tico
def _generate_montos_maximos_response(query: str, search_results: List[Dict]) -> str:
    """PROBLEMA: Ignora completamente search_results"""
    return """üìã **MONTOS M√ÅXIMOS DIARIOS DE VI√ÅTICOS MINEDU:**
    
üëë **ALTAS AUTORIDADES**
‚Ä¢ Ministros de Estado: S/ 380.00 soles    # ‚ùå INVENTADO
‚Ä¢ Viceministros: S/ 380.00 soles           # ‚ùå INVENTADO
"""

# REALIDAD en chunks procesados:
{
  "texto": "S/ 320.00 soles para funcionarios y directivos...",
  "metadatos": {"source": "directiva_viaticos.pdf"}
}
```

### üîç **Inconsistencias Identificadas**
- **Chunks procesados**: "S/ 320.00 soles"
- **Sistema responde**: "S/ 380.00 soles" 
- **Retrieval funciona**: Encuentra documentos relevantes
- **Generation falla**: Los ignora y responde hardcodeado

## üéØ ARQUITECTURA OBJETIVO: LANGCHAIN + LANGGRAPH

### **Arquitectura Actual (Problem√°tica)**
```
Usuario ‚Üí Frontend ‚Üí FastAPI ‚Üí HybridSearch ‚Üí [Documentos encontrados] 
                                                        ‚Üì
                                              [IGNORADOS] ‚Üí Respuesta hardcodeada
```

### **Arquitectura Propuesta (RAG Real)**
```
Usuario ‚Üí Frontend ‚Üí FastAPI ‚Üí LangGraph Orchestrator
                                      ‚Üì
                              Intent Classification
                                      ‚Üì
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ Agentes Especializados ‚îÇ
                            ‚îú‚îÄ ViaticosAgent (RAG) ‚îÄ‚î§
                            ‚îú‚îÄ IGVAgent (futuro) ‚îÄ‚îÄ‚îÄ‚î§
                            ‚îî‚îÄ GeneralAgent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚Üì
                              ChromaDB Retrieval
                                      ‚Üì
                            OpenAI GPT-4o-mini + Context
                                      ‚Üì
                            Respuesta basada en documentos REALES
```

## üîß ESPECIFICACIONES T√âCNICAS

### **Stack Tecnol√≥gico Nuevo**
```yaml
Framework: LangChain ^0.1.0          # RAG profesional
Orchestration: LangGraph ^0.1.0      # Multiagente
VectorDB: ChromaDB ^0.4.0           # Vector store local
LLM: OpenAI gpt-4o-mini             # Cost-effective
Embeddings: text-embedding-3-small   # Embeddings eficientes
```

### **Preservar Infraestructura Existente**
```yaml
Frontend: Next.js 14                 # ‚úÖ Mantener
Backend: FastAPI                     # ‚úÖ Evolucionar 
Chunks: chunks.json                  # ‚úÖ Migrar a ChromaDB
Vectorstores: *.pkl                  # ‚úÖ Conservar como fallback
```

### **Nueva Estructura de Archivos**
```
backend/src/
‚îú‚îÄ‚îÄ main.py                          # EVOLUCIONAR: H√≠brido
‚îî‚îÄ‚îÄ langchain_integration/           # NUEVO
    ‚îú‚îÄ‚îÄ config.py                    # Configuraci√≥n LangChain
    ‚îú‚îÄ‚îÄ agents/
    ‚îÇ   ‚îú‚îÄ‚îÄ viaticos_agent.py       # Agente RAG especializado
    ‚îÇ   ‚îú‚îÄ‚îÄ igv_agent.py            # Agente IGV (Fase 2)
    ‚îÇ   ‚îî‚îÄ‚îÄ base_agent.py           # Agente base
    ‚îú‚îÄ‚îÄ vectorstores/
    ‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py      # Migraci√≥n chunks ‚Üí ChromaDB
    ‚îÇ   ‚îî‚îÄ‚îÄ chroma_manager.py       # Gesti√≥n ChromaDB
    ‚îî‚îÄ‚îÄ orchestration/
        ‚îú‚îÄ‚îÄ orchestrator.py         # LangGraph orchestrator
        ‚îî‚îÄ‚îÄ intent_classifier.py    # Clasificaci√≥n intenci√≥n
```

## ü§ñ AGENTE VI√ÅTICOS ESPECIALIZADO

### **ViaticosAgent con RAG Real**
```python
class ViaticosAgent:
    """Agente especializado en normativa de vi√°ticos con RAG verdadero"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1,  # Precisi√≥n legal m√°xima
            max_tokens=1000
        )
        
        self.vectorstore = ChromaDB(
            persist_directory="./data/vectorstores/chromadb",
            embedding_function=OpenAIEmbeddings()
        )
        
        # Prompt especializado para precisi√≥n legal
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un especialista legal en normativa de vi√°ticos del MINEDU.

REGLAS CR√çTICAS:
1. SOLO responde bas√°ndote en el contexto de documentos proporcionado
2. Para montos, fechas y n√∫meros: copia EXACTAMENTE del documento  
3. Si la informaci√≥n no est√° en contexto, responde: "No encontr√© esa informaci√≥n en la normativa consultada"
4. Cita SIEMPRE el art√≠culo, directiva o fuente espec√≠fica
5. Mant√©n precisi√≥n legal absoluta - ni inventes ni interpretes

CONTEXTO DE DOCUMENTOS:
{context}

METADATOS DE FUENTES:
{metadata}"""),
            ("human", "{query}")
        ])
    
    async def process_query(self, query: str) -> Dict[str, Any]:
        """Procesar consulta con RAG real"""
        
        # 1. RETRIEVAL: Buscar documentos relevantes
        documents = self.vectorstore.similarity_search(
            query, 
            k=5  # Top 5 documentos m√°s relevantes
        )
        
        if not documents:
            return {
                "response": "No encontr√© informaci√≥n relevante en los documentos disponibles.",
                "sources": [],
                "confidence": 0.0
            }
        
        # 2. AUGMENTATION: Preparar contexto
        context = "\n\n".join([
            f"DOCUMENTO {i+1}:\n{doc.page_content}"
            for i, doc in enumerate(documents)
        ])
        
        metadata = [
            {
                "titulo": doc.metadata.get('titulo', 'Sin t√≠tulo'),
                "source": doc.metadata.get('source', 'Fuente no especificada'),
                "page": doc.metadata.get('page', 0)
            }
            for doc in documents
        ]
        
        # 3. GENERATION: Generar respuesta basada en contexto
        response = self.llm.invoke(
            self.prompt.format_messages(
                query=query,
                context=context,
                metadata=str(metadata)
            )
        )
        
        return {
            "response": response.content,
            "sources": metadata,
            "confidence": 0.95 if len(documents) >= 3 else 0.7,
            "documents_found": len(documents),
            "method": "langchain_rag"
        }
```

## üîÑ MIGRACI√ìN DE DATOS

### **Chunks Existentes ‚Üí ChromaDB**
```python
class DocumentMigrator:
    """Migra chunks.json existentes a ChromaDB de LangChain"""
    
    def migrate_chunks(self, chunks_path: str = "data/processed/chunks.json") -> Chroma:
        """Migraci√≥n completa preservando metadatos"""
        
        # 1. Cargar chunks existentes
        with open(chunks_path, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        # 2. Convertir a Documents de LangChain
        documents = []
        for chunk in chunks_data:
            doc = Document(
                page_content=chunk.get('texto', ''),
                metadata={
                    'id': chunk.get('id', ''),
                    'titulo': chunk.get('titulo', ''),
                    'source': chunk.get('metadatos', {}).get('source', ''),
                    'page': chunk.get('metadatos', {}).get('page', 0),
                    'type': chunk.get('metadatos', {}).get('type', ''),
                    'section': chunk.get('metadatos', {}).get('section', ''),
                    # Preservar metadatos originales completos
                    'original_metadata': chunk.get('metadatos', {})
                }
            )
            documents.append(doc)
        
        # 3. Crear vectorstore con embeddings OpenAI
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
            persist_directory="./data/vectorstores/chromadb",
            collection_name="minedu_documents"
        )
        
        logger.info(f"Migraci√≥n completada: {len(documents)} documentos ‚Üí ChromaDB")
        return vectorstore
```

## üé≠ ORQUESTADOR LANGGRAPH

### **Flujo Multiagente**
```python
class MINEDUOrchestrator:
    """Orquestador principal usando LangGraph"""
    
    def _create_graph(self) -> StateGraph:
        """Crear grafo de orquestaci√≥n"""
        
        workflow = StateGraph(OrchestrationState)
        
        # Nodos del flujo
        workflow.add_node("analyze_intent", self._analyze_intent)
        workflow.add_node("route_agents", self._route_to_agents)
        workflow.add_node("synthesize", self._synthesize_response)
        workflow.add_node("handle_error", self._handle_error)
        
        # Flujo principal
        workflow.set_entry_point("analyze_intent")
        
        # Routing condicional
        workflow.add_conditional_edges(
            "analyze_intent",
            self._should_continue,
            {
                "route_agents": "route_agents",
                "handle_error": "handle_error"
            }
        )
        
        workflow.add_edge("route_agents", "synthesize") 
        workflow.add_edge("synthesize", END)
        workflow.add_edge("handle_error", END)
        
        return workflow.compile()
    
    def _analyze_intent(self, state: OrchestrationState) -> OrchestrationState:
        """An√°lisis de intenci√≥n (Fase 1: keywords, Fase 2: LLM)"""
        query = state["query"].lower()
        
        # Intent detection por keywords (Fase 1)
        if any(kw in query for kw in ["vi√°tico", "viatico", "monto", "declaraci√≥n jurada"]):
            state["intent"] = "viaticos"
        elif any(kw in query for kw in ["igv", "impuesto", "tributo"]):
            state["intent"] = "igv"
        else:
            state["intent"] = "general"
            
        return state
    
    def _route_to_agents(self, state: OrchestrationState) -> OrchestrationState:
        """Routing a agentes especializados"""
        intent = state.get("intent", "general")
        query = state["query"]
        
        agent_responses = {}
        
        if intent == "viaticos":
            # Procesar con agente de vi√°ticos (RAG real)
            import asyncio
            response = asyncio.run(viaticos_agent.process_query(query))
            agent_responses["viaticos"] = response
        
        # TODO Fase 2: Agregar m√°s agentes
        # elif intent == "igv":
        #     response = await igv_agent.process_query(query)
        
        state["agent_responses"] = agent_responses
        return state
```

## üîó INTEGRACI√ìN FASTAPI H√çBRIDA

### **Endpoint H√≠brido con Fallback**
```python
# backend/src/main.py - Modificaciones

# Nuevos imports
try:
    from langchain_integration.orchestration.orchestrator import orchestrator
    LANGCHAIN_AVAILABLE = True
    logger.info("‚úÖ LangChain integration cargada")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è LangChain no disponible: {e}")
    LANGCHAIN_AVAILABLE = False

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """Endpoint h√≠brido: LangChain primero, fallback al sistema actual"""
    
    start_time = time.time()
    
    # 1. INTENTAR LANGCHAIN ORCHESTRATOR PRIMERO
    if LANGCHAIN_AVAILABLE and orchestrator:
        try:
            logger.info("üöÄ Usando LangChain orchestrator")
            
            langchain_result = orchestrator.process_query(request.message)
            
            if langchain_result.get("success", False):
                return {
                    "response": langchain_result.get("response", ""),
                    "conversation_id": request.conversation_id or f"conv_{int(time.time())}",
                    "timestamp": datetime.now().isoformat(),
                    "sources": langchain_result.get("sources", []),
                    "processing_time": round(time.time() - start_time, 3),
                    "method": "langchain_rag",
                    "intent": langchain_result.get("intent", ""),
                    "success": True
                }
            else:
                logger.warning("‚ùå LangChain fall√≥, usando fallback")
                
        except Exception as e:
            logger.error(f"‚ùå Error en LangChain: {e}")
    
    # 2. FALLBACK AL SISTEMA ACTUAL
    logger.info("üîÑ Usando sistema h√≠brido como fallback")
    
    # Buscar documentos con sistema existente
    search_results = hybrid_search.search(request.message)
    
    # Generar respuesta con sistema actual (como respaldo)
    return _generate_chat_response(request, search_results, start_time)

# Endpoints adicionales para testing
@app.post("/api/chat/langchain")
async def chat_langchain_direct(request: ChatRequest):
    """Endpoint directo para testing LangChain"""
    if not LANGCHAIN_AVAILABLE:
        raise HTTPException(status_code=503, detail="LangChain no disponible")
    
    result = orchestrator.process_query(request.message)
    return result

@app.post("/api/admin/migrate-to-langchain")
async def migrate_to_langchain():
    """Migrar chunks existentes a LangChain"""
    if not LANGCHAIN_AVAILABLE:
        raise HTTPException(status_code=503, detail="LangChain no disponible")
    
    from langchain_integration.vectorstores.document_loader import document_migrator
    
    vectorstore = document_migrator.migrate_chunks()
    
    return {
        "message": "Migraci√≥n completada",
        "vectorstore_path": config.chroma_persist_directory,
        "timestamp": datetime.now().isoformat()
    }
```

## üìä PLAN DE VALIDACI√ìN

### **Tests de Consistencia Cr√≠ticos**
```python
# Validaci√≥n que el sistema responda con contenido real
test_cases = [
    {
        "query": "¬øCu√°l es el monto m√°ximo de vi√°ticos para funcionarios?",
        "expected_in_response": "S/ 320.00",      # Lo que dicen chunks reales
        "forbidden_in_response": "S/ 380.00",     # Lo que inventa sistema actual
        "source_validation": "chunks.json ‚Üí ChromaDB"
    },
    {
        "query": "¬øQu√© documentos se requieren para declaraci√≥n jurada?",
        "method": "extract_from_real_documents",
        "validate_sources": True
    }
]

def test_langchain_consistency():
    """Validar que LangChain responda con datos reales"""
    for case in test_cases:
        response = orchestrator.process_query(case["query"])
        
        # Verificar contenido esperado presente
        assert case["expected_in_response"] in response["response"]
        
        # Verificar contenido inventado ausente  
        assert case["forbidden_in_response"] not in response["response"]
        
        # Verificar fuentes reales citadas
        assert len(response["sources"]) > 0
```

## üìà M√âTRICAS Y ROI

### **KPIs Fase 1**
| M√©trica | Sistema Actual | Objetivo LangChain | Mejora |
|---------|----------------|-------------------|---------|
| **Precisi√≥n** | ~40% | >95% | +137% |
| **Consistencia chunks ‚Üî respuestas** | 0% | 100% | +‚àû% |
| **Latencia** | ~2s | <5s | Aceptable |
| **Escalabilidad** | 5 docs | 3,000+ docs | +59,900% |
| **Mantenimiento** | Alto | Bajo | -70% |

### **Costos Estimados**
```yaml
Desarrollo: $0                    # Preserva infraestructura
OpenAI API: $20-150/mes          # Dependiente de uso
Infraestructura: $0               # Usa actual
Total mes: $20-150               # vs $300K+ enterprise

ROI estimado:
- Ahorro mantenimiento: $350/mes
- Mejora productividad: +200%
- ROI primer a√±o: +2,800%
```

## üöÄ ROADMAP DE IMPLEMENTACI√ìN

### **Fase 1: RAG Real (2-3 semanas)**
- [ ] Instalar dependencias LangChain
- [ ] Configurar OpenAI API Key
- [ ] Migrar chunks.json ‚Üí ChromaDB
- [ ] Implementar ViaticosAgent
- [ ] Endpoint h√≠brido con fallback
- [ ] **Validar**: Respuestas = contenido real

### **Fase 2: Multiagentes (1-2 meses)**
- [ ] LangGraph orchestrator completo
- [ ] IGVAgent, ProcedimientosAgent
- [ ] Intent classification por LLM
- [ ] Routing inteligente
- [ ] S√≠ntesis multi-agente

### **Fase 3: Escalado (3-6 meses)**
- [ ] Optimizaci√≥n 3,000+ documentos
- [ ] Caching y cost optimization
- [ ] Monitoring y observabilidad
- [ ] CI/CD deployment autom√°tico

## üéØ CRITERIOS DE √âXITO

### **Test Cr√≠tico de Consistencia**
```bash
# Antes (Sistema actual)
curl -X POST "localhost:8001/api/chat" -d '{"message": "monto m√°ximo vi√°ticos"}'
# Respuesta: "S/ 380.00" ‚ùå INVENTADO

# Despu√©s (LangChain)  
curl -X POST "localhost:8001/api/chat" -d '{"message": "monto m√°ximo vi√°ticos"}'
# Respuesta: "S/ 320.00" ‚úÖ EXTRA√çDO DE CHUNKS REALES
```

### **Validaci√≥n de √âxito**
- ‚úÖ Sistema responde solo con informaci√≥n de documentos reales
- ‚úÖ Montos extra√≠dos = montos en chunks (100% consistencia)
- ‚úÖ Fuentes citadas verificables en ChromaDB
- ‚úÖ Latencia < 5 segundos por consulta  
- ‚úÖ Fallback funciona si LangChain falla
- ‚úÖ Tests automatizados passing

---

**üéØ Esta migraci√≥n convierte un sistema "amateur" con respuestas inventadas en un sistema "profesional" con RAG real, preservando 100% de la inversi√≥n actual.** 