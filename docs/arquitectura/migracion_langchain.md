# ðŸ—ï¸ Arquitectura de MigraciÃ³n LangChain/LangGraph

> **Documento**: EspecificaciÃ³n tÃ©cnica de migraciÃ³n  
> **Fecha**: 2025-07-01  
> **Estado**: Planificado - Pendiente implementaciÃ³n  
> **Objetivo**: Migrar de sistema hardcodeado a RAG real  

## ðŸ“Š DIAGNÃ“STICO DEL PROBLEMA ACTUAL

### âŒ **Sistema con Respuestas Hardcodeadas**
```python
# backend/src/main.py - CÃ³digo problemÃ¡tico
def _generate_montos_maximos_response(query: str, search_results: List[Dict]) -> str:
    """PROBLEMA: Ignora completamente search_results"""
    return """ðŸ“‹ **MONTOS MÃXIMOS DIARIOS DE VIÃTICOS MINEDU:**
    
ðŸ‘‘ **ALTAS AUTORIDADES**
â€¢ Ministros de Estado: S/ 380.00 soles    # âŒ INVENTADO
â€¢ Viceministros: S/ 380.00 soles           # âŒ INVENTADO
"""

# REALIDAD en chunks procesados:
{
  "texto": "S/ 320.00 soles para funcionarios y directivos...",
  "metadatos": {"source": "directiva_viaticos.pdf"}
}
```

### ðŸ” **Inconsistencias Identificadas**
- **Chunks procesados**: "S/ 320.00 soles"
- **Sistema responde**: "S/ 380.00 soles" 
- **Retrieval funciona**: Encuentra documentos relevantes
- **Generation falla**: Los ignora y responde hardcodeado

## ðŸŽ¯ ARQUITECTURA OBJETIVO: LANGCHAIN + LANGGRAPH

### **Arquitectura Actual (ProblemÃ¡tica)**
```
Usuario â†’ Frontend â†’ FastAPI â†’ HybridSearch â†’ [Documentos encontrados] 
                                                        â†“
                                              [IGNORADOS] â†’ Respuesta hardcodeada
```

### **Arquitectura Propuesta (RAG Real)**
```
Usuario â†’ Frontend â†’ FastAPI â†’ LangGraph Orchestrator
                                      â†“
                              Intent Classification
                                      â†“
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Agentes Especializados â”‚
                            â”œâ”€ ViaticosAgent (RAG) â”€â”¤
                            â”œâ”€ IGVAgent (futuro) â”€â”€â”€â”¤
                            â””â”€ GeneralAgent â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                              ChromaDB Retrieval
                                      â†“
                            OpenAI GPT-4o-mini + Context
                                      â†“
                            Respuesta basada en documentos REALES
```

## ðŸ”§ ESPECIFICACIONES TÃ‰CNICAS

### **Stack TecnolÃ³gico Nuevo**
```yaml
Framework: LangChain ^0.1.0          # RAG profesional
Orchestration: LangGraph ^0.1.0      # Multiagente
VectorDB: ChromaDB ^0.4.0           # Vector store local
LLM: OpenAI gpt-4o-mini             # Cost-effective
Embeddings: text-embedding-3-small   # Embeddings eficientes
```

### **Preservar Infraestructura Existente**
```yaml
Frontend: Next.js 14                 # âœ… Mantener
Backend: FastAPI                     # âœ… Evolucionar 
Chunks: chunks.json                  # âœ… Migrar a ChromaDB
Vectorstores: *.pkl                  # âœ… Conservar como fallback
```

### **Nueva Estructura de Archivos**
```
backend/src/
â”œâ”€â”€ main.py                          # EVOLUCIONAR: HÃ­brido
â””â”€â”€ langchain_integration/           # NUEVO
    â”œâ”€â”€ config.py                    # ConfiguraciÃ³n LangChain
    â”œâ”€â”€ agents/
    â”‚   â”œâ”€â”€ viaticos_agent.py       # Agente RAG especializado
    â”‚   â”œâ”€â”€ igv_agent.py            # Agente IGV (Fase 2)
    â”‚   â””â”€â”€ base_agent.py           # Agente base
    â”œâ”€â”€ vectorstores/
    â”‚   â”œâ”€â”€ document_loader.py      # MigraciÃ³n chunks â†’ ChromaDB
    â”‚   â””â”€â”€ chroma_manager.py       # GestiÃ³n ChromaDB
    â””â”€â”€ orchestration/
        â”œâ”€â”€ orchestrator.py         # LangGraph orchestrator
        â””â”€â”€ intent_classifier.py    # ClasificaciÃ³n intenciÃ³n
```

## ðŸ¤– AGENTE VIÃTICOS ESPECIALIZADO

### **ViaticosAgent con RAG Real**
```python
class ViaticosAgent:
    """Agente especializado en normativa de viÃ¡ticos con RAG verdadero"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1,  # PrecisiÃ³n legal mÃ¡xima
            max_tokens=1000
        )
        
        self.vectorstore = ChromaDB(
            persist_directory="./data/vectorstores/chromadb",
            embedding_function=OpenAIEmbeddings()
        )
        
        # Prompt especializado para precisiÃ³n legal
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un especialista legal en normativa de viÃ¡ticos del MINEDU.

REGLAS CRÃTICAS:
1. SOLO responde basÃ¡ndote en el contexto de documentos proporcionado
2. Para montos, fechas y nÃºmeros: copia EXACTAMENTE del documento  
3. Si la informaciÃ³n no estÃ¡ en contexto, responde: "No encontrÃ© esa informaciÃ³n en la normativa consultada"
4. Cita SIEMPRE el artÃ­culo, directiva o fuente especÃ­fica
5. MantÃ©n precisiÃ³n legal absoluta - ni inventes ni interpretes

CONTEXTO DE DOCUMENTOS:
{context}

METADATOS DE FUENTES:
{metadata}"""),
            ("human", "{query}")
        ])
    
    async def process_query(self, query: str) -> Dict[str, Any]:
        """Procesar consulta con RAG real"""
        
        # 1. RETRIEVAL: Buscar documentos relevantes
        documents = self.vectorstore.similarity_search(
            query, 
            k=5  # Top 5 documentos mÃ¡s relevantes
        )
        
        if not documents:
            return {
                "response": "No encontrÃ© informaciÃ³n relevante en los documentos disponibles.",
                "sources": [],
                "confidence": 0.0
            }
        
        # 2. AUGMENTATION: Preparar contexto
        context = "\n\n".join([
            f"DOCUMENTO {i+1}:\n{doc.page_content}"
            for i, doc in enumerate(documents)
        ])
        
        metadata = [
            {
                "titulo": doc.metadata.get('titulo', 'Sin tÃ­tulo'),
                "source": doc.metadata.get('source', 'Fuente no especificada'),
                "page": doc.metadata.get('page', 0)
            }
            for doc in documents
        ]
        
        # 3. GENERATION: Generar respuesta basada en contexto
        response = self.llm.invoke(
            self.prompt.format_messages(
                query=query,
                context=context,
                metadata=str(metadata)
            )
        )
        
        return {
            "response": response.content,
            "sources": metadata,
            "confidence": 0.95 if len(documents) >= 3 else 0.7,
            "documents_found": len(documents),
            "method": "langchain_rag"
        }
```

## ðŸ”„ MIGRACIÃ“N DE DATOS

### **Chunks Existentes â†’ ChromaDB**
```python
class DocumentMigrator:
    """Migra chunks.json existentes a ChromaDB de LangChain"""
    
    def migrate_chunks(self, chunks_path: str = "data/processed/chunks.json") -> Chroma:
        """MigraciÃ³n completa preservando metadatos"""
        
        # 1. Cargar chunks existentes
        with open(chunks_path, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        # 2. Convertir a Documents de LangChain
        documents = []
        for chunk in chunks_data:
            doc = Document(
                page_content=chunk.get('texto', ''),
                metadata={
                    'id': chunk.get('id', ''),
                    'titulo': chunk.get('titulo', ''),
                    'source': chunk.get('metadatos', {}).get('source', ''),
                    'page': chunk.get('metadatos', {}).get('page', 0),
                    'type': chunk.get('metadatos', {}).get('type', ''),
                    'section': chunk.get('metadatos', {}).get('section', ''),
                    # Preservar metadatos originales completos
                    'original_metadata': chunk.get('metadatos', {})
                }
            )
            documents.append(doc)
        
        # 3. Crear vectorstore con embeddings OpenAI
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
            persist_directory="./data/vectorstores/chromadb",
            collection_name="minedu_documents"
        )
        
        logger.info(f"MigraciÃ³n completada: {len(documents)} documentos â†’ ChromaDB")
        return vectorstore
```

## ðŸŽ­ ORQUESTADOR LANGGRAPH

### **Flujo Multiagente**
```python
class MINEDUOrchestrator:
    """Orquestador principal usando LangGraph"""
    
    def _create_graph(self) -> StateGraph:
        """Crear grafo de orquestaciÃ³n"""
        
        workflow = StateGraph(OrchestrationState)
        
        # Nodos del flujo
        workflow.add_node("analyze_intent", self._analyze_intent)
        workflow.add_node("route_agents", self._route_to_agents)
        workflow.add_node("synthesize", self._synthesize_response)
        workflow.add_node("handle_error", self._handle_error)
        
        # Flujo principal
        workflow.set_entry_point("analyze_intent")
        
        # Routing condicional
        workflow.add_conditional_edges(
            "analyze_intent",
            self._should_continue,
            {
                "route_agents": "route_agents",
                "handle_error": "handle_error"
            }
        )
        
        workflow.add_edge("route_agents", "synthesize") 
        workflow.add_edge("synthesize", END)
        workflow.add_edge("handle_error", END)
        
        return workflow.compile()
    
    def _analyze_intent(self, state: OrchestrationState) -> OrchestrationState:
        """AnÃ¡lisis de intenciÃ³n (Fase 1: keywords, Fase 2: LLM)"""
        query = state["query"].lower()
        
        # Intent detection por keywords (Fase 1)
        if any(kw in query for kw in ["viÃ¡tico", "viatico", "monto", "declaraciÃ³n jurada"]):
            state["intent"] = "viaticos"
        elif any(kw in query for kw in ["igv", "impuesto", "tributo"]):
            state["intent"] = "igv"
        else:
            state["intent"] = "general"
            
        return state
    
    def _route_to_agents(self, state: OrchestrationState) -> OrchestrationState:
        """Routing a agentes especializados"""
        intent = state.get("intent", "general")
        query = state["query"]
        
        agent_responses = {}
        
        if intent == "viaticos":
            # Procesar con agente de viÃ¡ticos (RAG real)
            import asyncio
            response = asyncio.run(viaticos_agent.process_query(query))
            agent_responses["viaticos"] = response
        
        # TODO Fase 2: Agregar mÃ¡s agentes
        # elif intent == "igv":
        #     response = await igv_agent.process_query(query)
        
        state["agent_responses"] = agent_responses
        return state
```

## ðŸ”— INTEGRACIÃ“N FASTAPI HÃBRIDA

### **Endpoint HÃ­brido con Fallback**
```python
# backend/src/main.py - Modificaciones

# Nuevos imports
try:
    from langchain_integration.orchestration.orchestrator import orchestrator
    LANGCHAIN_AVAILABLE = True
    logger.info("âœ… LangChain integration cargada")
except ImportError as e:
    logger.warning(f"âš ï¸ LangChain no disponible: {e}")
    LANGCHAIN_AVAILABLE = False

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """Endpoint hÃ­brido: LangChain primero, fallback al sistema actual"""
    
    start_time = time.time()
    
    # 1. INTENTAR LANGCHAIN ORCHESTRATOR PRIMERO
    if LANGCHAIN_AVAILABLE and orchestrator:
        try:
            logger.info("ðŸš€ Usando LangChain orchestrator")
            
            langchain_result = orchestrator.process_query(request.message)
            
            if langchain_result.get("success", False):
                return {
                    "response": langchain_result.get("response", ""),
                    "conversation_id": request.conversation_id or f"conv_{int(time.time())}",
                    "timestamp": datetime.now().isoformat(),
                    "sources": langchain_result.get("sources", []),
                    "processing_time": round(time.time() - start_time, 3),
                    "method": "langchain_rag",
                    "intent": langchain_result.get("intent", ""),
                    "success": True
                }
            else:
                logger.warning("âŒ LangChain fallÃ³, usando fallback")
                
        except Exception as e:
            logger.error(f"âŒ Error en LangChain: {e}")
    
    # 2. FALLBACK AL SISTEMA ACTUAL
    logger.info("ðŸ”„ Usando sistema hÃ­brido como fallback")
    
    # Buscar documentos con sistema existente
    search_results = hybrid_search.search(request.message)
    
    # Generar respuesta con sistema actual (como respaldo)
    return _generate_chat_response(request, search_results, start_time)

# Endpoints adicionales para testing
@app.post("/api/chat/langchain")
async def chat_langchain_direct(request: ChatRequest):
    """Endpoint directo para testing LangChain"""
    if not LANGCHAIN_AVAILABLE:
        raise HTTPException(status_code=503, detail="LangChain no disponible")
    
    result = orchestrator.process_query(request.message)
    return result

@app.post("/api/admin/migrate-to-langchain")
async def migrate_to_langchain():
    """Migrar chunks existentes a LangChain"""
    if not LANGCHAIN_AVAILABLE:
        raise HTTPException(status_code=503, detail="LangChain no disponible")
    
    from langchain_integration.vectorstores.document_loader import document_migrator
    
    vectorstore = document_migrator.migrate_chunks()
    
    return {
        "message": "MigraciÃ³n completada",
        "vectorstore_path": config.chroma_persist_directory,
        "timestamp": datetime.now().isoformat()
    }
```

## ðŸ“Š PLAN DE VALIDACIÃ“N

### **Tests de Consistencia CrÃ­ticos**
```python
# ValidaciÃ³n que el sistema responda con contenido real
test_cases = [
    {
        "query": "Â¿CuÃ¡l es el monto mÃ¡ximo de viÃ¡ticos para funcionarios?",
        "expected_in_response": "S/ 320.00",      # Lo que dicen chunks reales
        "forbidden_in_response": "S/ 380.00",     # Lo que inventa sistema actual
        "source_validation": "chunks.json â†’ ChromaDB"
    },
    {
        "query": "Â¿QuÃ© documentos se requieren para declaraciÃ³n jurada?",
        "method": "extract_from_real_documents",
        "validate_sources": True
    }
]

def test_langchain_consistency():
    """Validar que LangChain responda con datos reales"""
    for case in test_cases:
        response = orchestrator.process_query(case["query"])
        
        # Verificar contenido esperado presente
        assert case["expected_in_response"] in response["response"]
        
        # Verificar contenido inventado ausente  
        assert case["forbidden_in_response"] not in response["response"]
        
        # Verificar fuentes reales citadas
        assert len(response["sources"]) > 0
```

## ðŸ“ˆ MÃ‰TRICAS Y ROI

### **KPIs Fase 1**
| MÃ©trica | Sistema Actual | Objetivo LangChain | Mejora |
|---------|----------------|-------------------|---------|
| **PrecisiÃ³n** | ~40% | >95% | +137% |
| **Consistencia chunks â†” respuestas** | 0% | 100% | +âˆž% |
| **Latencia** | ~2s | <5s | Aceptable |
| **Escalabilidad** | 5 docs | 3,000+ docs | +59,900% |
| **Mantenimiento** | Alto | Bajo | -70% |

### **Costos Estimados**
```yaml
Desarrollo: $0                    # Preserva infraestructura
OpenAI API: $20-150/mes          # Dependiente de uso
Infraestructura: $0               # Usa actual
Total mes: $20-150               # vs $300K+ enterprise

ROI estimado:
- Ahorro mantenimiento: $350/mes
- Mejora productividad: +200%
- ROI primer aÃ±o: +2,800%
```

## ðŸš€ ROADMAP DE IMPLEMENTACIÃ“N

### **Fase 1: RAG Real (2-3 semanas)**
- [ ] Instalar dependencias LangChain
- [ ] Configurar OpenAI API Key
- [ ] Migrar chunks.json â†’ ChromaDB
- [ ] Implementar ViaticosAgent
- [ ] Endpoint hÃ­brido con fallback
- [ ] **Validar**: Respuestas = contenido real

### **Fase 2: Multiagentes (1-2 meses)**
- [ ] LangGraph orchestrator completo
- [ ] IGVAgent, ProcedimientosAgent
- [ ] Intent classification por LLM
- [ ] Routing inteligente
- [ ] SÃ­ntesis multi-agente

### **Fase 3: Escalado (3-6 meses)**
- [ ] OptimizaciÃ³n 3,000+ documentos
- [ ] Caching y cost optimization
- [ ] Monitoring y observabilidad
- [ ] CI/CD deployment automÃ¡tico

## ðŸŽ¯ CRITERIOS DE Ã‰XITO

### **Test CrÃ­tico de Consistencia**
```bash
# Antes (Sistema actual)
curl -X POST "localhost:8001/api/chat" -d '{"message": "monto mÃ¡ximo viÃ¡ticos"}'
# Respuesta: "S/ 380.00" âŒ INVENTADO

# DespuÃ©s (LangChain)  
curl -X POST "localhost:8001/api/chat" -d '{"message": "monto mÃ¡ximo viÃ¡ticos"}'
# Respuesta: "S/ 320.00" âœ… EXTRAÃDO DE CHUNKS REALES
```

### **ValidaciÃ³n de Ã‰xito**
- âœ… Sistema responde solo con informaciÃ³n de documentos reales
- âœ… Montos extraÃ­dos = montos en chunks (100% consistencia)
- âœ… Fuentes citadas verificables en ChromaDB
- âœ… Latencia < 5 segundos por consulta  
- âœ… Fallback funciona si LangChain falla
- âœ… Tests automatizados passing

---

**ðŸŽ¯ Esta migraciÃ³n convierte un sistema "amateur" con respuestas inventadas en un sistema "profesional" con RAG real, preservando 100% de la inversiÃ³n actual.** 